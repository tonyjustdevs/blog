{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"A Basic NLP model - [Competition Version]\"\n",
    "author: \"Tony Phung\"\n",
    "date: \"2024-02-17\"\n",
    "categories: [NLP, kaggle]\n",
    "image: \"patent.jpg\"\n",
    "toc: true\n",
    "description: \"Training my first NLP Model\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](patent.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "From [Part 1](https://tonyjustdevs.github.io/blog/posts/kaggle/1-us-patents/), the Kaggle Competition: **[U.S. Patent Phrase to Phrase Matching ](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching)** was a **notebook competition**, that is, a simple csv upload of predictions would not suffice.\n",
    "\n",
    "A **notebook** with code needs to be submitted in order to succesfully enter and be graded.\n",
    "\n",
    "This notebook will have access the kaggles cloud folders to gather the raw data, process and model it. The only catch is the notebook has **no access to the internet**. This means `pip install package` will not work.   \n",
    "\n",
    "Thus, the pre-trained models and tokenizers need to be uploaded to the inputs folder, before being installed.\n",
    "\n",
    "Why? Even though `transformers` library is available on Kaggle via `import`, each time a function like `AutoTokenizer` is called, this accesses the internet to reach the [HuggingFace Model Hub](https://huggingface.co/docs/hub/en/models-the-hub) and looks for the latest available models.\n",
    "\n",
    "In order to upload files though, they need to be exported first, but in order for them to be exported, they need to be downloaded first!. Well that is what I figured out, there's probably a vastly more seemless way but I didn't go out of my way to find out a way to do it, this way just made the most sense. In the future, I'll find out a better way.\n",
    "\n",
    "So, the idea is:  \n",
    "1. create new kaggle noteook with internet access  \n",
    "2. install libraries  \n",
    "3. run our models  \n",
    "4. export our models  \n",
    "5. download our libraries  \n",
    "6. create new kaggle noteook with no internet access  \n",
    "7. upload downloaded libraries and exported models  \n",
    "8. install libraries via uploaded files  \n",
    "9. import pre-trained models vias uploaded files  \n",
    "10. conduct training  \n",
    "11. make predictions\n",
    "12. export to csv  \n",
    "13. submit notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Export Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debv3_tokenizer.save_pretrained(\"./tokenizer\")\n",
    "model.save_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](model_tokenizer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Libraries\n",
    "\n",
    "Two libraries are required for this notebook to run `datasets` and `transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip download datasets\n",
    "!pip download transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload File to Kaggle\n",
    "### 4.1 Gather files\n",
    "\n",
    "Place all json files (tokenizer and model) and whl files (libraries) in the same folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](json_whl.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Kaggle Upload\n",
    "\n",
    "1. Go to **[Datasets]**\n",
    "2. Then **[New Dataset]**\n",
    "3. Name a **[Dataset Title]**\n",
    "4. Choose all **files** from your local folder\n",
    "5. Click **[Create]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](kgl_upload.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Load Succesful\n",
    "\n",
    "Upon completion, a greeting of success should appear.\n",
    "\n",
    "![](success.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Add Data\n",
    "\n",
    "- Click **[Add Data]**\n",
    "- Filter for **[Your Datasets]**\n",
    "- Find the uploaded **Dataset**\n",
    "\n",
    "![](add_data.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Code\n",
    "\n",
    "It's almost the same code as the previous post so I've combined it altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-index --find-links=. transformers\n",
    "!pip install --no-index --find-links=. datasets\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,TrainingArguments,Trainer\n",
    "from pathlib import Path\n",
    "from datasets import Dataset,DatasetDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "model_nm = \"microsoft/deberta-v3-small\"\n",
    "path = Path('/kaggle/input/us-patent-phrase-to-phrase-matching')\n",
    "mypath = Path('/kaggle/input/us-patents-libraries-model-tokenizer')\n",
    "tokenizer_uploaded  = AutoTokenizer.from_pretrained(mypath)\n",
    "model_uploaded = AutoModelForSequenceClassification.from_pretrained(mypath)\n",
    "df = pd.read_csv(path/'train.csv')\n",
    "df.describe(include='object')\n",
    "df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n",
    "ds = Dataset.from_pandas(df)\n",
    "def tok_func(x): return tokenizer_uploaded(x[\"input\"])\n",
    "tok_ds = ds.map(tok_func, batched=True)\n",
    "tok_ds = tok_ds.rename_columns({'score':'labels'})\n",
    "dds = tok_ds.train_test_split(0.25, seed=42)\n",
    "eval_df = pd.read_csv(path/'test.csv')\n",
    "eval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor\n",
    "eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)\n",
    "def corr(x,y): return np.corrcoef(x,y)[0][1]\n",
    "def corr_d(eval_pred): return {'pearson': corr(*eval_pred)}\n",
    "bs = 128\n",
    "epochs = 2\n",
    "lr = 8e-5\n",
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(mypath, num_labels=1,ignore_mismatched_sizes=True)\n",
    "trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "                  tokenizer=tokenizer_uploaded, compute_metrics=corr_d)\n",
    "trainer.train()\n",
    "preds = trainer.predict(eval_ds).predictions.astype(float)\n",
    "preds = np.clip(preds, 0, 1)\n",
    "submission = datasets.Dataset.from_dict({\n",
    "    'id': eval_ds['id'],\n",
    "    'score': preds.squeeze()\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "\n",
    "It worked!\n",
    "\n",
    "![](results.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
