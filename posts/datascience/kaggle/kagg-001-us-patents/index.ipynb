{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"KAGG 1: A Basic NLP model\"\n",
    "author: \"Tony Phung\"\n",
    "date: \"2024-02-16\"\n",
    "categories: [kaggle, datascience, nlp]\n",
    "image: \"nlp.jpg\"\n",
    "toc: true\n",
    "description: \"Training my first NLP Model\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import a Pretrained Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_pretrained_model = \"microsoft/deberta-v3-small\"\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "debv3_tokenizer = AutoTokenizer.from_pretrained(chosen_pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](import_tok.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Look Inside the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(debv3_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](inside_tok.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test out Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = (\"Hey all! What's going on? It's Tony from Sydney!\")\n",
    "debv3_tokenizer.tokenize(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](test_tok.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Competition Data\n",
    "\n",
    "To add relevant competition data to your kaggle \"Input\" folder. \n",
    "\n",
    "This \"Input\" folder is persistent when you submit to the competition. All other folders created during prior to submitting are disregarded.\n",
    "\n",
    "### 3.1 Via GUI:\n",
    "1. On Kaggle, Go to **[Add Data]**  \n",
    "2. Filter for \"**Competition Datasets**\"  \n",
    "3. Search \"**US Patents**\"\n",
    "4. Click **[Add Competition]**\n",
    "\n",
    "![](add_uspatent_data.jpg)\n",
    "![](input_uspatent_data.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Via Programatically:\n",
    "*Note: You'll need your own GPU's, I don't so the rest of the notebook is ran on the Kaggle website*\n",
    "1. Have kaggle login + keys ready locally, explained in this [post](https://tonyjustdevs.github.io/blog/posts/2024-01-27-99_kaggle_api/)\n",
    "2. Run code to download data locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path = Path('us-patent-phrase-to-phrase-matching')\n",
    "if not path.exists():\n",
    "    import zipfile,kaggle\n",
    "    kaggle.api.competition_download_cli(str(path))\n",
    "    zipfile.ZipFile(f'{path}.zip').extractall(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Look Inside the Competition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/kaggle/input/us-patent-phrase-to-phrase-matching') # Using GUI places comp-data into 'kaggle/input' folder\n",
    "import pandas as pd\n",
    "df = pd.read_csv(path/'train.csv')\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](df_describe.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "\n",
    "### 4.1 Create Input Column\n",
    "Create a contentated column of imporatant columns `context`, `target` and `anchor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n",
    "df['input']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](hf_input.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Convert Pandas Dataframe to HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset,DatasetDict\n",
    "hf_datasets = Dataset.from_pandas(df)\n",
    "hf_datasets.keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](hf_ds.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Tokenize our HuggingFace Dataset \n",
    "\n",
    "Using the tokenizer, we can apply pre-trained model to our new concatenated column.\n",
    "\n",
    "A hugging face dataset is in the form of a dictionary so we can index to get a column with `dict['column']`\n",
    "\n",
    "We can apply the tokenization with batching, resulting in an additional few columns `input_ids`, `token_type_ids`, `attention_marks`, which only took 2 seconds!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_func(x): return debv3_tokenizer(x[\"input\"])\n",
    "tok_ds = hf_datasets.map(tok_func, batched=True)\n",
    "tok_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tok_ds.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Rename the Columns as to what HF expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds = tok_ds.rename_columns({'score':'labels'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Training and Validation Sets\n",
    "\n",
    "Split the above tokenized hugging face datasets into validation and training sets, into `DatasetDict`s.\n",
    "\n",
    "***Note***: The validation set here is called **test** and **not validate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds_dicts = tok_ds.train_test_split(0.25, seed=42)\n",
    "tok_ds_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tok_ds_dict.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Modelling\n",
    "### 5.1 Import libraries and set parameters\n",
    "\n",
    "Import modules: \n",
    "- `TrainingArgument`: to take in all the hyperparameters\n",
    "- `Trainer` class: combines the TrainingArguments and Pre-trained model\n",
    "Set the main hyper-parameters:\n",
    "- **Batch Sizes**: to fit on the GPU, \n",
    "- **Number of Epochs**: for each 'experiment' and the\n",
    "- **Learning Rate**, so it doesnt fail.\n",
    "\n",
    "**[\"Future Iteration\"]**: More descriptions on these and other parameters in future posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments,Trainer\n",
    "bs = 128\n",
    "epochs = 4\n",
    "lr = 8e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Setup Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(chosen_pretrained_model, \n",
    "                                                           num_labels=1,\n",
    "                                                           ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Create Metrics Functions\n",
    "\n",
    "The Pearson coefficient using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def corr(x,y): return np.corrcoef(x,y)[0][1]\n",
    "def corr_d(eval_pred): return {'pearson': corr(*eval_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, \n",
    "                  args, \n",
    "                  train_dataset=tok_ds_dicts['train'], \n",
    "                  eval_dataset=tok_ds_dicts['test'],\n",
    "                  tokenizer=debv3_tokenizer, \n",
    "                  compute_metrics=corr_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Do the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](4_epochs.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predictions\n",
    "\n",
    "Now that we have a Trainer (same as Learner in FastAI), we could use it on a an unseen set of data such as a Test Set and make predictions.\n",
    "\n",
    "### 6.1 Import Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(path/'test.csv')\n",
    "eval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor\n",
    "eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Make Predictions\n",
    "\n",
    "Predictions are going beyond 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(eval_ds).predictions.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](preds.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Clip Predictions\n",
    "\n",
    "Predictions are going beyond 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.clip(preds, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](predclip.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "submission = datasets.Dataset.from_dict({\n",
    "    'id': eval_ds['id'],\n",
    "    'score': preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Part 2\n",
    "\n",
    "Actually the submissiong won't work because it is a **Notebook** competition is the **Internet is Turned Off**.  \n",
    "\n",
    "What needs to be done is convert this version to one that works without installing anything from the internet.  \n",
    "\n",
    "That would be in Part 2.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
