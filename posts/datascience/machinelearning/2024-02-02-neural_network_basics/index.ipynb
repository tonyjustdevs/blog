{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Neural Network Basics (Part 2)\"\n",
    "author: \"Tony Phung\"\n",
    "date: \"2024-02-02\"\n",
    "categories: [machinelearning, ai, mathematics]\n",
    "image: \"mae.jpg\"\n",
    "toc: true\n",
    "description: \"Optimising with Gradient Descent\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automation of finding the best parameters (lowest loss) based on Mean Average Error (MAE) using **Gradient Descent** for our Quadratic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from fastai.basics import *\n",
    "import pandas as pd\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Data and Convert Data to Pytorch **Tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"upload_dataset.csv\")\n",
    "x_trch = torch.tensor(df.x) \n",
    "y_trch = torch.tensor(df.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Customisable Quadratic functions and Interactively Plot with MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_quad_fn(a,b,c,x): return a*x**2 + b*x + c\n",
    "def custom_quad_fn(a,b,c): return partial(gen_quad_fn,a,b,c)\n",
    "def torch_mae(prediction, actual): return (torch.abs(prediction-actual).mean())\n",
    "def torch_mse(prediction, actual): return ((prediction-actual)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mae(prediction, actual): return np.mean(abs(prediction-actual))\n",
    "# def torch_mae(prediction, actual): return np.mean(torch.abs(prediction-actual))\n",
    "# def mae(prediction, actual): return (torch.abs(prediction-actual).mean())\n",
    "# def mae2(prediction, actual): return abs(prediction-actual).mean()\n",
    "# def mae_jh(prediction, actual): return (abs(prediction-actual)).mean()\n",
    "# def mse_jh(prediction, actual): return ((prediction-actual)**2).mean()\n",
    "# def mae(preds, acts): return (torch.abs(preds-acts)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d74afc6403c43619b24f44acec5706d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='a', max=2.1), FloatSlider(value=1.0, description='b'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rc('figure', dpi=90)\n",
    "\n",
    "@interact(a=(0,2.1,0.1),b=(0,2.1,0.1),c=(0,2.1,0.1))\n",
    "def interactive_plot(a,b,c):\n",
    "# 1.    plot scatter\n",
    "    plt.scatter(x_trch, y_trch)\n",
    "# 2     create custom_quad_interactive_fn\n",
    "# 2.1   create xs_interact    \n",
    "    xs_interact = x_trch\n",
    "# 3.    create ys_interact\n",
    "    plt.ylim(-1,15)\n",
    "    ys_interact = custom_quad_fn(a,b,c)(xs_interact)\n",
    "# 4.    calc mae\n",
    "    y_actual     = y_trch\n",
    "    y_predicted  = custom_quad_fn(a,b,c)(x_trch)\n",
    "    interact_mae = torch_mae(y_predicted,y_actual)\n",
    "# 5. plot   \n",
    "    plt.plot(xs_interact, ys_interact)\n",
    "    plt.title(f\"MAE: {interact_mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Determining the effect of the parameters ($a$, $b$, $c$) in:  $ax + bx^2 + c$\n",
    "\n",
    "The key thing to understand if whether the loss function gets better or worse when you increase the parameters a little.\n",
    "\n",
    "There are two ways we can try:\n",
    "1. **Manually** adjust the parameter: Move each parameter each way and observe the impact to MAE.  \n",
    "2. Calculate the **Derivative** of the parameter: A Derivative iS a function that tells you if you increase the input the: \n",
    "    - **direction** in which output changes (increases or decreases) and the;  \n",
    "    - **magnitude** of the change to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create Mean-Absolute-Error (mae) Quadratic Function\n",
    "This function will take in the parameters or coefficients of a quadratic function and output the MSE.\n",
    "- *Input*: coeffiicents of quadratic\n",
    "- *Output*: MAE (between the prediction of the quadratic with the coffecients of the quadratic and the actual predictsions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_quad_fn(x_trch, y_trch, abc_params):\n",
    "    quad_fn = custom_quad_fn(*abc_params)\n",
    "    y_predicted_trch = quad_fn(x_trch)\n",
    "    y_actual_trch    = y_trch\n",
    "    # so quad_params(2,3,4) ->  creates a custom quad fn -> 2x^2 + 3x + 4\n",
    "    return torch_mae(y_predicted_trch,y_actual_trch)\n",
    "\n",
    "def mse_quad_fn(x_trch, y_trch, abc_params):\n",
    "    quad_fn = custom_quad_fn(*abc_params)\n",
    "    y_predicted_trch = quad_fn(x_trch)\n",
    "    y_actual_trch    = y_trch\n",
    "    # so quad_params(2,3,4) ->  creates a custom quad fn -> 2x^2 + 3x + 4\n",
    "    return torch_mse(y_predicted_trch,y_actual_trch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart shows MAE(2,2,2) = 1.4501 loss\n",
    "Our mae_function also calculates 1.491 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6103, dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_quad_fn(x_trch=x_trch,y_trch=y_trch,abc_params=[1.0,1.0,1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **tensor** is a pytorch type that works with: \n",
    "- lists (1D tensors)\n",
    "- tables (2D tensors)\n",
    "- layers of tables of numbers (3D tensors) and etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Telling PyTorch to calculate gradients\n",
    "\n",
    "By calling method .requires_grad_(), our `abc_rg` tensor is not will calculate gradients whenever we use the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rank 1 tensor\n",
    "abc_rg = torch.tensor([1.0,1.0,1.0])\n",
    "abc_rg.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc_rg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Method .requires_grad_() \n",
    "`grad_fn=<MeanBackward0>` shows the gradients are calculated to for each parameter (our inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6103, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mae_quad_fn(x_trch, y_trch, abc_rg)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Method .backward()\n",
    "This adds an attribute .grad to our abc_rg tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Attribute .grad\n",
    "\n",
    "This attributes tells us if we increase the input slightly in the same position of this tensor, the loss will increase (if its positive) or decrease (if negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3529, -0.0316, -0.5000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc_rg.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4 Increase our `abc` parameters and recalculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before: 2.61030324932801\n",
      "loss after: 2.5894896953092177\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(f\"loss before: {loss}\")\n",
    "    abc_rg -= abc_rg.grad * 0.01\n",
    "    loss = mae_quad_fn(x_trch, y_trch, abc_rg)\n",
    "    print(f\"loss after: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.2.5 Automate it\n",
    "\n",
    "Create a loop that decreases the loss by iteratively increasing the parameters (since the gradients are negative, or vice versa) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 2.5894896953092177 - tensor([-2.7058, -0.0632, -1.0000])\n",
      "step 1: 2.547862587271633 - tensor([-4.0587, -0.0947, -1.5000])\n",
      "step 2: 2.4854217639359875 - tensor([-5.4116, -0.1263, -2.0000])\n",
      "step 3: 2.4021673865815485 - tensor([-6.7645, -0.1579, -2.5000])\n",
      "step 4: 2.2980994552083187 - tensor([-8.1175, -0.1895, -3.0000])\n",
      "step 5: 2.173217969816296 - tensor([-9.4704, -0.2211, -3.5000])\n",
      "step 6: 2.0300959430578267 - tensor([-10.6892,  -0.3684,  -3.9000])\n",
      "step 7: 1.883669135864714 - tensor([-11.9080,  -0.5158,  -4.3000])\n",
      "step 8: 1.740979068220988 - tensor([-12.9396,  -0.8000,  -4.6000])\n",
      "step 9: 1.5914231086209807 - tensor([-13.9712,  -1.0842,  -4.9000])\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    loss = mae_quad_fn(x_trch, y_trch, abc_rg)\n",
    "    loss.backward()\n",
    "    with torch.no_grad(): abc_rg -= abc_rg.grad * 0.01\n",
    "    print(f\"step {i}: {loss} - {abc_rg.grad}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5 Parameters are getting closer\n",
    "\n",
    "The parameters started as `1,1,1` and now are `1.9, 1.0, 1.3`, the underlying function was modelled with `3, 2, 1` so its getting there!\n",
    "\n",
    "**[Future Iteration]** How to just fix a parameter and just move the others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8739, 1.0365, 1.3170], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc_rg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be Continued...  \n",
    "\n",
    "Next A universal function called the **ReLU Function** (rather than a quadratric function) is used for our modelling.  \n",
    "\n",
    "[Neural Network Basics: Part 1](https://tonyjustdevs.github.io/blog/posts/2024-01-31-99_neural_network_basics/)  \n",
    "[Neural Network Basics: Part 2](https://tonyjustdevs.github.io/blog/posts/2024-02-02-neural_network_basics/)   \n",
    "[Neural Network Basics: Part 3](https://tonyjustdevs.github.io/blog/posts/2024-02-03-neural_network_basics/)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
