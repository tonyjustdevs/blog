{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Measuring Model Accuracy\"\n",
    "author: \"Tony Phung\"\n",
    "date: \"2024-04-22\"\n",
    "categories: [coding, metrics]\n",
    "image: \"imgs/6acc.jpg\"\n",
    "toc: true\n",
    "description: \"Step-by-step guide on how to measure accuracy of a deeplearning model\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/6acc.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Methodology\n",
    "\n",
    "1. Calculate `Predictions` with our model `Coefficients` with our `Independent Variables (Validation Set)`.\n",
    "    - Expected output: `Float Values (between 0 and 1)`.\n",
    "2. Convert Predictions to `True` if above 0.5 otherwise `False`.\n",
    "    - Expected output: `Boolean Values (True and False)`.\n",
    "3. Compare `Booled Predictions` to `Dependent Variables (Validation Set)`\n",
    "    - Expected output: `Boolean Values (True and False)`.   \n",
    "4. Convert the `Boolean` values to `Float`.\n",
    "    - Expected output: `Integer Values (1s and 0s)`.\n",
    "5. Calculate `Mean` of the floated values.\n",
    "    - Expected output: `Single Float Value (between 0 and 1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 2. Run Deep-Learning Model and Get Coefficients\n",
    "\n",
    "See previous blog [post](https://tonyjustdevs.github.io/blog/posts/2024-04-21-deep_learning/) for model and code explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titanic exists!\n",
      "gender_submission.csv  test.csv  train.csv\n",
      "0.521;0.483;0.427;0.379;0.379;0.379;0.379;0.378;0.378;0.378;0.378;0.378;0.378;0.378;0.378;0.378;0.377;0.376;0.371;0.333;0.239;0.224;0.208;0.204;0.203;0.203;0.207;0.197;0.196;0.195;"
     ]
    }
   ],
   "source": [
    "import kaggle, zipfile\n",
    "from pathlib import Path\n",
    "import torch, numpy as np, pandas as pd\n",
    "from fastai.data.transforms import RandomSplitter\n",
    "import torch.nn.functional as F\n",
    "path = Path(\"titanic\")\n",
    "if not path.exists():\n",
    "    print(f\"{path} folder doesn't exist, downloading...\")\n",
    "    kaggle.api.competition_download_cli(str(path))\n",
    "    zipfile.ZipFile(f\"{path}.zip\").extractall(path)\n",
    "else:\n",
    "    print(f\"{path} already exists, using this folder...\")\n",
    "!ls {path}\n",
    "df = pd.read_csv(path/\"train.csv\")\n",
    "def df_1_fillna_inplace(df):\n",
    "    modes = df.mode(axis=0).iloc[0] # get modes\n",
    "    df.fillna(modes, inplace=True)  # replace nas with mode per col\n",
    "def df_2_log_numeric_data_addlogfare(df): df['LogFare'] = np.log1p(df['Fare'])\n",
    "def df_3_create_dummy_variables_add(df):\n",
    "    return pd.get_dummies(df, columns=[\"Sex\", \"Pclass\", \"Embarked\"], dtype=int)\n",
    "def df_clean(df):\n",
    "    df_1_fillna_inplace(df)\n",
    "    df_2_log_numeric_data_addlogfare(df)\n",
    "    return df_3_create_dummy_variables_add(df)\n",
    "\n",
    "def get_idep_and_dep_from_df(df):\n",
    "    def normalise_idep_by_max(idep):\n",
    "        maxes, _ = idep.max(axis=0) # get max of each column\n",
    "        return idep / maxes \n",
    "    \n",
    "    added_cols          = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n",
    "    indep_cols          = ['Age', 'SibSp', 'Parch', 'LogFare'] + added_cols\n",
    "    idep                = torch.tensor(df[indep_cols].values, dtype=torch.float)\n",
    "    idep                = normalise_idep_by_max(idep)\n",
    "    dep                 = torch.tensor(df[\"Survived\"])\n",
    "    return idep, dep\n",
    "\n",
    "def get_trn_val_idep_dep(idep, dep):     \n",
    "    trn_idx, val_idx            = RandomSplitter(seed=42)(idep)\n",
    "    trn_dep_mx0,  val_dep_mx0   = dep[trn_idx], dep[val_idx] # 1-dimension i.e. cant matrix multiply \n",
    "    trn_idep_mxn, val_idep_mxn  = idep[trn_idx], idep[val_idx] \n",
    "    trn_dep_mx1                 = trn_dep_mx0[:,None] # add extra dimention for matrix multiply\n",
    "    val_dep_mx1                 = val_dep_mx0[:,None]\n",
    "    return trn_idep_mxn, val_idep_mxn, trn_dep_mx1, val_dep_mx1 \n",
    "df = df_clean(df)\n",
    "idep, dep = get_idep_and_dep_from_df(df)\n",
    "trn_idep_mxn, val_idep_mxn, trn_dep_mx1, val_dep_mx1 = get_trn_val_idep_dep(idep, dep)\n",
    "def init_coeffs():\n",
    "    n_coeffs    = trn_idep_mxn.shape[1] # 12\n",
    "    hidden_layers = [10,10]\n",
    "    sizes = [n_coeffs] + hidden_layers + [1]    # [12,10,10,1]\n",
    "    layers = [(torch.rand(sizes[i],sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(len(sizes)-1)]   # 0,1,2\n",
    "    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(len(sizes)-1)]   # [0,1,2]\n",
    "    for layer in layers+consts:\n",
    "        layer.requires_grad_()\n",
    "    return layers, consts\n",
    "def calc_preds_deeplearning(trn_idep_mxn, coeffs):    \n",
    "    layers, consts = coeffs\n",
    "    n = len(layers)\n",
    "    res = trn_idep_mxn\n",
    "    for i in range(n):\n",
    "        res = res@layers[i] + consts[i] # [mxn]@[nxq]  [713x12][12x10]\n",
    "        if i!=n-1: \n",
    "            res = F.relu(res) \n",
    "    sgm_preds_mx1 = torch.sigmoid(res)\n",
    "    return sgm_preds_mx1\n",
    "def calc_loss(idep_mxn, dep_mx1, coeffs):\n",
    "    preds_mx1 = calc_preds_deeplearning(idep_mxn, coeffs)\n",
    "    return torch.abs(dep_mx1-preds_mx1).mean()\n",
    "def update_coeffs(coeffs, lr):\n",
    "    layers, consts = coeffs\n",
    "    for layer in layers+consts:\n",
    "        layer.sub_(layer.grad*lr)\n",
    "        layer.grad.zero_()\n",
    "def one_epoch(coeffs,lr):\n",
    "    loss = calc_loss(trn_idep_mxn, trn_dep_mx1, coeffs)\n",
    "    loss.backward()\n",
    "    with torch.no_grad(): update_coeffs(coeffs, lr)\n",
    "    print(f\"{loss:.3f}\",end=';')\n",
    "def train_model(n_epochs=30,lr=0.1):\n",
    "    torch.manual_seed(442)\n",
    "    coeffs = init_coeffs()\n",
    "    for _ in range(n_epochs):\n",
    "        one_epoch(coeffs,lr)\n",
    "    return coeffs\n",
    "coeffs = train_model(lr=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8258)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_accuracy_deepelearning(val_idep_mxn, coeffs):\n",
    "    val_preds_mx1               = calc_preds_deeplearning(val_idep_mxn, coeffs)     # 1.\n",
    "    bool_preds_mx1              = val_preds_mx1>0.5                                 # 2. \n",
    "    comp_dep_vs_preds_val_mx1   = (val_dep_mx1==bool_preds_mx1)                     # 3.\n",
    "    float_comp_mx1              = comp_dep_vs_preds_val_mx1.float()                 # 4.   \n",
    "    accuracy_mx1                = float_comp_mx1.mean()                             # 5. \n",
    "    return accuracy_mx1\n",
    "\n",
    "calculate_accuracy_deepelearning(val_idep_mxn, coeffs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
