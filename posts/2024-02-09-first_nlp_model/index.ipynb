{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"A Basic NLP model\"\n",
    "author: \"Tony Phung\"\n",
    "date: \"2024-02-09\"\n",
    "categories: [NLP]\n",
    "image: \"nlp.jpg\"\n",
    "toc: true\n",
    "description: \"Training my first NLP Model\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlpcup.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What's a Pre-trained Model\n",
    "\n",
    "Recall in [Neural Network Basics](https://tonyjustdevs.github.io/blog/posts/2024-01-31-99_neural_network_basics/), A line was fit to a set of data by adjusting the parameters of a quadratic function:\n",
    "\n",
    "![](mae.jpg)\n",
    "\n",
    "\n",
    "A **Pre-Trained Model** is being like being told the coefficients that the coefficient of `a` is definitely 3 and `b` is around 2. \n",
    "\n",
    "**Fine-Tuning** is process of finding the coefficients that are not known, in this case, `c`, whilst slightly moving `a` and `b`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ULMFiT: 3 Stages of Transfer Learning in NLP\n",
    "\n",
    "The **ULMFiT** process is shown in the diagram below:\n",
    "\n",
    "![](ulm.jpg)\n",
    "\n",
    "1. Build **Language Model 1 Wikipedia**:\n",
    "- *Goal*: To predict the next word (or every next word) of every wikipedia article.\n",
    "- *Process*: Started with random weights\n",
    "- *Acheivement*: a model that can predict >30% correclty the next word in a wikipedia article.\n",
    "\n",
    "2. Build **Language Model 2 IMDb**:\n",
    "- *Goal*: Predict next word of IMDb movie review.\n",
    "- *Process*: Use Pre-trained model (i.e. its weights) from 1, run a few more epochs (fine-tune) with IMDb movie reviews\n",
    "- *Achievement*: Predicted with high accuracy next word of IMbd review.\n",
    "\n",
    "2. Build **IMDb Movie Review Classifier**:\n",
    "- *Goal*: Classify whether an IMDb movie review is positive or negative sentiment\n",
    "- *Process*: Use pre-trained model (i.e. its weights ) from 2 and fine-tune.\n",
    "- *Achievement*: Classify IMbd reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Background: Inside the Layers\n",
    "\n",
    "Zeiler and Fergus paper highlights:\n",
    "\n",
    "High level\n",
    "1. Layer 1: sets of weights that found diagonal edges, ie **edge detectors*\n",
    "2. Layer 2: combined layer 1 ReLUs added together, sets of output so of those ReLUS, i.e Activations, then run through a MM ReLU, resulting in **corner detectors**\n",
    "3. Layer 5: Bird and Lizard eyeball detectors, dog face dectors, flower detector and etc\n",
    "\n",
    "**ResNet50** has 50 layer.\n",
    "\n",
    "The **latter layers** do things more specific to the training task, ie predicting whats being looked at.\n",
    "\n",
    "Whilst the **early layers**, don't change much as they're generalised for most natural photos, sharp and rounded edges etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformers Model (a different architecture)\n",
    "\n",
    "Built to take advantage of accelerators (Googles TPUs).  \n",
    "Unlike ULMFit, Transformers didn't predict the next word of the sentence. \n",
    "\n",
    "However, it was trained by taking texts and deleting at random a few words and asked the model what were the models removed.\n",
    "\n",
    "Comparisons to ULMfit:\n",
    "- RNN vs Transformer model\n",
    "- Language model vs Masked Language model\n",
    "\n",
    "Hence, this post will focus on the more popular method, from the Transformer Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. The notebook\n",
    "# path\n",
    "# !kaggle kernels pull -p kg -m \"jhoward/getting-started-with-nlp-for-absolute-beginners\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Kaggle Competition - US Patent Phrase-to-Phrase Matching\n",
    "**Goal**: Make predictions with my own basic NLP model and make a succesful submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Set up API keys\n",
    "Setting up the API Keys explained [in a previous post](https://tonyjustdevs.github.io/blog/posts/2024-01-27-99_kaggle_api/).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "creds = ''\n",
    "cred_path = Path('~/.kaggle/kaggle.json').expanduser()\n",
    "if not cred_path.exists():\n",
    "    cred_path.parent.mkdir(exist_ok=True)\n",
    "    cred_path.write_text(creds)\n",
    "    cred_path.chmod(0o600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Setup Paths\n",
    "\n",
    "Ensure correct competition name is set to path :`us-patent-phrase-to-phrase-matching`, otherwise data wont be found.  \n",
    "\n",
    "Easy way is just the string after `competitions/` in the URL:  \n",
    "\n",
    "![](compname.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('us-patent-phrase-to-phrase-matching')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('us-patent-phrase-to-phrase-matching')\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Download Competition Datasets\n",
    "\n",
    "A folder will be created with related csvs files:  \n",
    "\n",
    "![](uspfolder.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
    "\n",
    "if not iskaggle and not path.exists():\n",
    "    import zipfile,kaggle\n",
    "    kaggle.api.competition_download_cli(str(path))\n",
    "    zipfile.ZipFile(f'{path}.zip').extractall(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Look at Data\n",
    "\n",
    "Open up `train.csv` and take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>36473</td>\n",
       "      <td>733</td>\n",
       "      <td>29340</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>component composite coating</td>\n",
       "      <td>composition</td>\n",
       "      <td>H01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>24</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                       anchor       target context\n",
       "count              36473                        36473        36473   36473\n",
       "unique             36473                          733        29340     106\n",
       "top     37d61fd2272659b1  component composite coating  composition     H01\n",
       "freq                   1                          152           24    2186"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(path/'train.csv')\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modelling\n",
    "### 6.1 What is the problem to be solved?\n",
    "\n",
    "**Goal**: Given two words or short phrases, make a prediction (scoring) them on **similiarity**.\n",
    "- `1`: identifiy meanings\n",
    "- `0`: totally different meanings\n",
    "- `0.5`: somewhat similar but not identical.\n",
    "\n",
    "**Process**: Represent as a **classification problem** by ***concatenating*** various important columns into a text separated by its constant header:\n",
    "- **txt1**: `context`  \n",
    "- **txt2**: `target`   \n",
    "- **txt3**: `anchor`    \n",
    "- **category of similarity**: One of 3 categories [**different**; **similar**; **identical**];\n",
    "\n",
    "Thus, the cateogry of similar is the what is to be predicted for unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Take a look at the new column `input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    TEXT1: A47; TEXT2: abatement of pollution; ANC...\n",
       "1    TEXT1: A47; TEXT2: act of abating; ANC1: abate...\n",
       "2    TEXT1: A47; TEXT2: active catalyst; ANC1: abat...\n",
       "3    TEXT1: A47; TEXT2: eliminating process; ANC1: ...\n",
       "4    TEXT1: A47; TEXT2: forest region; ANC1: abatement\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks don't accept text, it accets numbers as inputs. Text needs to coverted in two methods:\n",
    "\n",
    "[Stage 1]: **Tokenization**: Split each text up into words (into *tokens*)  \n",
    "[Stage 2]: **Numericalization**: Convert each word (or token) into a number.\n",
    "\n",
    "Notes:\n",
    "- A token is the general concept of what each split is, rather than words.\n",
    "- The Tokenization stage involves many decisions that need to be made. \n",
    "    - These decisions are made by the pre-trained model needs to be followed exactly in order to have the exacty same vocabulary.\n",
    "    - Before tokenizing, the decision to choose what model to use must be made.\n",
    "    - The huggingface model hub has thousands of pre-trained models:\n",
    "\n",
    "![](models.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],\n",
       "    num_rows: 36473\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset,DatasetDict\n",
    "hf_ds = Dataset.from_pandas(df) #pd df to hf df\n",
    "hf_ds # shows the features and number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Question]** is there a graph to show which models are best, fastest, most accurate for NLP, like they did for CV on timms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Pick a Language model\n",
    "\n",
    "Picking a model is important because we must **tokenize** in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14812/3658171669.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_nm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rand(notes) \n",
    "- nn = numbers * matrices -> apply relu -> add up -> repeat\n",
    "- after splitting -> list of unique words = vocab\n",
    "    - each new world -> a number\n",
    "- bigger vocab -> more memory -> more data to train (we wont want vc to be too big)\n",
    "    - less memory - 'tokenize' with sub-words (hugging face txformers and dataset to do it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
