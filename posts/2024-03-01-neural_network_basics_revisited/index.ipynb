{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Neural Networks Basics (Revisited) - [DRAFT TBC 02/3/24]\"\n",
    "author: \"Tony Phung\"\n",
    "date: \"2024-03-01\"\n",
    "categories: [deep dive, laymens]\n",
    "image: \"deep.jpg\"\n",
    "toc: true\n",
    "description: \"Deeper dive into Neural Network basics\"\n",
    "---\n",
    "\n",
    "***POSTING ON LIVE BLOG TO FORCE MYSELF TO FINISH THIS POST ASAP***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](deep.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to dive into previously explored topics a bit more **deeply**. Time to try to understand what's actually going on.\n",
    "\n",
    "These posts help me trigger what kind of thought process I had at the time of learning the topic. It's **not be easily disgestable for other people üó∫Ô∏è** but feel free to read on.\n",
    "\n",
    "Up til now, I've sped-run topics whilst not understanding much of the  details. \n",
    "\n",
    "I like to get the code running at least and produce results to expectation before getting in the weeds. At least I know the code works so I don't end up heading towards doom üåö. \n",
    "\n",
    "In these deeper dives:    \n",
    "\n",
    "- Lets hope to do these concepts justice (I'll try to get feedback from professionals at some point). Feel free to correct my understanding! (I'm sure some parts aren't on the ball üëéüçô).  \n",
    "\n",
    "- The concepts and math are brand new to me (or done a decade ago), these posts help me express things in my way, which is usually the very lay way. It is a pretty raw thoughts at each step of the concepts and ideas I'm trying to understand; Con-steps? Step-deas? (Sorry ‚ùÑÔ∏è).  \n",
    "\n",
    "- Also, without writing them down, I'll end up forgetting most of how and what I thought to conceive these concepts (I need to improve my vocabulary), i.e. it will be like these thoughts had never had happened (loss and irretrievable from the black-hole that is my mind ‚ö´).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction  \n",
    "### 1.1 The Mission\n",
    "\n",
    "![](mission.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Mission]**:  \n",
    "\n",
    "- Create a `model` that takes in some `inputs` and provides a reasonble `output` (prediction).\n",
    "\n",
    "**[Method]**:  \n",
    "\n",
    "\n",
    "- A `mathematical function` is a form of a model:\n",
    "    - takes in inputs `x1, x2, ...` and \n",
    "    - `transforms` it into a single output `y = F(x1, x2, ...)`. \n",
    "\n",
    "This seems like a good appraoch to tackle the mission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The Mathematical Model\n",
    "`Create a Linear Model` (or mathematical function) given some `(real world) data` (data that represents something we want to predict given similar input) in the form $$y = a_1x_1 + a_2x_2 + ... a_nx_n$$\n",
    "It is called `Linear` because our inputs `x` are of degree 1.  \n",
    "\n",
    "- Each `inputs` (`x1, x2, ..., xn`) is multiplied by their corresponding `coefficients` or `parameters` (`a1, a2, ..., an`), i.e.: $$a_1x_1 + a_2x_2 + ...$$ \n",
    "    - The `inputs` variables `x`'s are like features or characteristics of our model.\n",
    "    - the `coefficients` (known as `parameters`) in machine learning talk, scale the features/inputs. By scaling the inputs, it's like finding out which input/feature matters more or not, in determining the output. \n",
    "- The output `y` (known as `predictions`) is calculated by `summing all the scaled parameters` together: `a1x1 + a2x2 + ...`: $$F(x) = y = a_1x_1 + a_2x_2 + ... + a_nx_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Laymens (mathematical model):  \n",
    "- If a coefficient $a_1, a_2 ...$ is **almost zero**, it's like saying it does not impact the prediction value (remember we are adding up scaled versions of inputs to calculate the output / prediction). Then thats like saying perhaps this particualr parameter is an unimportant variable (or feature)! \n",
    "    - Perhaps we can get rid of it altogether? \n",
    "    - Less variables, less calculations, less overhead (and less work!) and a more simple model without losing significant predictive power.\n",
    "- If it has a large coefficient, then it would impact the overall sum, hence prediction. Probably shouldnt disregard this parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Simple Examples (of mathematical models)\n",
    "Two simple examples (single input `x` and a single output `y`) can be visualised on the `2D-plane`:\n",
    "\n",
    "- A `straight line` where input `x` is the horizontal-axis and output: `F(x)` and `y` is on the vertical-axis is with the formula we all know and love: $$ F(x) = mx + b $$\n",
    "\n",
    "And similarly,  \n",
    "\n",
    "- A `quadratic line` is: $$F(x) = ax^2 + bx + c$$\n",
    "\n",
    "**Note**: ***Our model should `generalise a mathematical function` (or find out the core characteristics and relationships ) given our set of data, so once we know that, we can predict unseen data with similar traits.***\n",
    "\n",
    "For a quadratic, what combination of $a$, $b$ and $c$ will help us predict closely the value we want given some random $x$ and then another $x$, in fact, how about all future values of $x$ as we want and have good predictions all of them on average $x$? That's what we are trying to do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Steps  \n",
    "\n",
    "### 2.1 [Step 1]: Choose (Create) the General Quadratic Equation/Model  \n",
    "\n",
    "Assume a quadratic equation: `F(x) = ax^2 + bx + c` can help us model some real world phenomena (e.g. throwing a ball or driving then stopping).  \n",
    "\n",
    "![](quad_real_world.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 [Step 2]: Make Predictions!\n",
    "\n",
    "Okay I wish it was that easy, but ***But what does making predictions mean and how do you do it***?\n",
    "\n",
    "Actually, in this context, it's as simple as plugging in the different input values and their coefficients (`a,b,c,x`) to our quadaratic equation to see what output value (`y`) we get.    \n",
    "\n",
    "In other words, calculate the `F(x)` (predictions) by using different combinations of our inputs `x` and parameters `a,b,c` with the equation `F(x) = ax^2 + bx + c`. Simple!\n",
    "\n",
    "Wait no, but how do we know what `input variables x` and combinations `coefficients a,b,c` to use? Surely not just anything random? Kinda yes but also no.\n",
    "\n",
    "So in order to make useful predictions, we should have **systematic way** to decide what ***`starting`*** coefficients and variables to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 **LONG Laymens** on using good ***starting*** coefficients and variables  \n",
    "\n",
    "A `Good` set of inputs: \n",
    "\n",
    "- Say we want to produce a model that guesses whether a passenger on the Titanic **Survived** and **Didnt Survive**.  \n",
    "    - A pretty good set of input variables would be data on actual passengers of the titanic and on whether they survived or not.   \n",
    "    - A model could learn what characteristics of these passengers and then adjust the parameters, say `gender` or `age` to see if the prediction is better or worse, knowing already the survivability of the passenger in the first place.\n",
    "\n",
    "A `Bad` set of inputs:\n",
    "\n",
    "- would be perhaps characteristics about passengers wouldnt probably not impact their survivability like:   \n",
    "    - `their favourite colour` or   \n",
    "    - `their dominant hand` or  \n",
    "    - `ear, finger, arm, or toe lengths` (within each group of adults and kids)\n",
    "\n",
    "An `even worse` set of inputs (moving towards the bad `validation set` territory, more on this later).  \n",
    "\n",
    "- would be getting data from unrelated sources (and sorry to state the obvious), such as:   \n",
    "    - the cohort of 2015 Applied Finance Graduates from Macquarie University, Sydney,  \n",
    "    - a group people picked at random off streets of Binh Thanh Saigon, Vietnam,  \n",
    "    - a group of online matches from a popular dating app obtained by a user.  \n",
    "These groups  technically surivived the Titanic so the model will say all their characteristics of everyone are useful, which isnt a great predictor (it'll predict being born after the Titanic sunk gives you a 100% chance of surviving the Titanic ü§≠)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For coefficients, I heard its ***more of an art than a science*** on what are good starting parameter values (although I assume later there are some cool algorithms that give us a hand in the future).    \n",
    "\n",
    "- Logically it would make sense that initially **input dominate parameters** (input values are large relative to their coefficients) and then they're scaled by a **parameter** according to importance through the neural network.  \n",
    "- This means, start with **relatively low value parameters** (to it's input variables) and let these coefficients adjust (`learn` through `gradient descent`) to larger values, whether positive or negative.  \n",
    "- In other words start with coefficients **about zero**.   \n",
    "- Okay that kinda makes sense but lets talk through an example:   \n",
    "    -   Lets assume we have a parameters should be relatively large and positive (for e.g. being a female or child in first class on the Titanic is probably two good parameters for survivability).  \n",
    "    - If the parameter starts off large and negative then this causes the model to take alot of iterations to learn and adjust the parameter to be positive. And vice versa.  \n",
    "    - So if we set all near zero, it will simply take `less time on average` for most models for each parameter to get to whether they need to go (`optimisation?`) considering we don't know what parameters matter or not, so its random for us, ie, it can be positive or negative, we just don't know (***assuming there is no super obvious characteristic that would dominate a model, in which case, maybe its better to put a larger value? perhaps that isnt a data science way to do things though, I'm not sure yet***).  \n",
    "        - From my understanding, by initially `setting the some coefficients initially` and then adjust the model to our data, this is actually called `learning with a pre-trained model` (not set by us). The pre-trained model has already been run and complete by someone else and we are teaching the model learn about our data on top of its knowledge.  \n",
    "        - Most of what I'll be initially doing is `Building a Learner from a Pre-trained Model`. In the future, I hope to learn to build the Pre-Trained models from scratch, if its a useful endeavour?     \n",
    "        - Popular examples of pre-trained models are `ResNet50`, which are trained on (millions of) pictures with 50 layers deep neural network.  \n",
    "- Another example to drive it home to start between `-0.5 to 0.5`:  \n",
    "    - Say we play a game where I tell you to choose a starting value and take count by 1 towards my guess and you want to choose a starting value to minimise the count on average.   \n",
    "    - And I tell you my guess is always random and between `-50 and 50`.   \n",
    "    - You'd probably pick zero because its common sense! (I hope it is)  \n",
    "    - It's kind of same in our model, except we dont pick zero itself because it ruins the math (explain later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***That sounds well and good but:***\n",
    "\n",
    "1. Doesnt a coefficients **relativeness** depend on the actual value of its **corresponding input parameter**. So shouldnt the starting coefficient be set **relative to its corresponding starting input parameter**, and not just an `arbitrary range of -0.5 to 0.5`? and also\n",
    "2. Input parameters can **range drastically** **within itself** and **relative to other variables**! For example:\n",
    "- `age` could be range `from 1 to 130` years old   \n",
    "- `Amount of money fundraised or donated` by people could range `0 to billions`  \n",
    "- `Lenght of the petals of a plum flower` could range `0.1 mm to 40 mm`.  \n",
    "- `x-values` of any line function could be negative $\\infty$ to positive $\\infty$ (ngl I wanted to use the $\\infty$ sign for once in my life ‚ôæÔ∏è)  \n",
    "\n",
    "***How does it make sense to just arbitrarily choose -0.5 to 0.5 then?***  \n",
    "\n",
    "` insert explanation of normalisation of data `  \n",
    "` insert explanation of dummy variables `  \n",
    "\n",
    "Thus, I'll use randomly generated values between -0.5 and +0.5 (-1.0 and +1.0 could also work)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The Real Step 2: 2a Set the Parameters `a, b, and c` \n",
    "- **[Action]**: Set our parameters to be random numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Step 2b: Input `x` \n",
    "Set a range values input values.\n",
    "   \n",
    "- **Note**: In real world and here there is always a set of real world data. \n",
    "- Unfortunately computers can really interpret photos or audio or text in its aesthestic form like humans. Fortunately, these different modes can be represented with numbers which computers are very good at handling.\n",
    "    - A **picture** are made up on pixels. Each pixel is a combination of a 3 parameters input (R,G,B) to create the colour.\n",
    "    - A piece of **Text** can be decomposed into words and letters and be labelled with integers too.\n",
    "\n",
    "For our model, given that we have some `real y-values` mapped to some `input x-values`, we should at least have predictions on those x-values to see how our predicted y-values are doing (known as `accuracy`, more later), calculate each `F(x1), F(x2), etc `, by going through each combination `a,b,c and x`:\n",
    "\n",
    "- `F(x1) = a1x2^2 + b1x1 + c1` (prediction 1 `y1 or F(x1) or F(x1,a1,b1,c1)` given this specific set of input variable `x1` and parameters `a1, b1, c1`)\n",
    "- `F(x2) = a2x2^2 + b2x2 + c2`, (prediction 2)... etc  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    " \n",
    "- These coefficients are started as `random` because we have to start somewhere. \n",
    "- Starting off random parameters is generally a good practice, the art is then to incrementally updating our coefficients (known as `gradient descent`) to make our model (`neural network`) do better (`improving accuracy` or `decreasing our loss function`) at each iteration (known as `epoch`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 TBA STEP Accuracy\n",
    "- `measure accuracy` of our predictions i.e. `Mean Absolute Error (MAE)`. \n",
    "    - For each actual output/value `y`, what is our `predicted y`.\n",
    "    - Since predictions can be `above or below` the actual value, we apply an absolute function to measure the difference or `loss`. That is, the MAE is also known as our `Loss Function`. An alternate popular loss function is the `Mean Squared Function (MSE)\n",
    "\n",
    "### 2.4 TBA STEP UPDATE PARAMETERS\n",
    "- `updating parameters` to \n",
    "    - `improve accuracy` of our predictions (i.e. decrease difference between our predictions and data)\n",
    "    - `do it automatically`: the art of improving our model or `learning` is called `gradient descent`.  \n",
    "- `neural network`: Once the model is accuracy we are happy with, this is our `neural network`.\n",
    "\n",
    "Quite simple really?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulate Data\n",
    "Simulate content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch\n",
    "\n",
    "np.random.seed(42)\n",
    "def noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\n",
    "def add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n",
    "\n",
    "actual_x_values_tsr = torch.linspace(-2, 2, steps=20)[:,None] # simulate 20 actual x-values + shape to 2D tensor\n",
    "\n",
    "def actual_function(x): return 3*x**2 + 2*x + 1 \n",
    "# In reality we don't have a real function like this to use, \n",
    "# however we use this + add noise, which simulates real data\n",
    "# then we try to model this noisey data\n",
    "actual_y_values_unrealistic_tsr = actual_function(actual_x_values_tsr)\n",
    " # actual y-values of funtion we use to find\n",
    "\n",
    "#  but again, these values are not realistic because they're based on the real function - something that doenst exist in real life,\n",
    "# its like having the exact function that determines whether a photo is a cat or not\n",
    "# we can only approximate functions and its parameters\n",
    "# in real world, data has noise,\n",
    "# introduce data to these unrealistic real values to product realistic actual values\n",
    "\n",
    "# okay so why dont we need to add noise to actual_x_values? its any input is realistic/real world\n",
    "# any photo is can be asked 'is it a cat?'\n",
    "# any passenger with any characters can be asked 'did the passenger survive?\n",
    "\n",
    "actual_y_values_realistic_tsr = add_noise(actual_y_values_unrealistic_tsr, 0.15, 1.5) # use actualy y-values + add noise - to create simulated real data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we have a set of (actual realistic) 'data'\n",
    "# 1. try 'model' it with a quadratic equation \n",
    "# 2. create loss function - mean absolute error - difference between each point of actual y vs predicted y at each x, find difference and then absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_x_values[:5],actual_y_values_realistic[:5]\n",
    "\n",
    "# 1. Create quad function with parameters\n",
    "def quad_fn(a,b,c,x): return a*x**2 + b*x + c\n",
    "y_a1_b1_c1_x1 = quad_fn(1,1,1,1)\n",
    "y_a1_b1_c1_x1  # 3 = (1*1^2) + (1*1) + (1) = 1+1+1\n",
    "y_a1_b1_c1_x2 = quad_fn(1,1,1,2)\n",
    "y_a1_b1_c1_x2  # 7 = (1*2^2) + (2*1) + (2) = 4+2+1\n",
    "\n",
    "# Its quite cumbersome to put a new x-value through the function, to get a corresponding predicted y-value\n",
    "# ideally we can provide a list of xs to get a list of predicted ys (x-tensor -> f -> y-tensor)\n",
    "# and also the coefficients are parameterised\n",
    "from functools import partial\n",
    "def mk_quad_fn(a,b,c): return partial(quad_fn,a,b,c)\n",
    "quad111 = mk_quad_fn(1,1,1)\n",
    "# quad111(actual_x_values)\n",
    "predicted_quad111_y_values_tsr = quad111(actual_x_values_tsr) # remember is a 2d tensor due to added dimension with: [:,None]\n",
    "\n",
    "# we have actual-yand predicted-y data, lets compare them\n",
    "\n",
    "def mae(actual, preds): return abs(preds-actual).mean()\n",
    "\n",
    "mae(actual_y_values_realistic_tsr,predicted_quad111_y_values_tsr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.plot(actual_y_values_realistic_tsr,predicted_quad111_y_values_tsr)\n",
    "plt.scatter(actual_x_values_tsr, actual_y_values_realistic_tsr)\n",
    "\n",
    "# plot predictions\n",
    "# for predictions, graphically it will look better to plot a line \n",
    "# rather than just a coressponding y-prediction to each actual y\n",
    "\n",
    "# lets do just corresponding ones first to see what it looks like\n",
    "\n",
    "plt.scatter(actual_x_values_tsr, predicted_quad111_y_values_tsr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(actual_x_values_tsr, actual_y_values_realistic_tsr)\n",
    "# plt.scatter(actual_x_values_tsr, predicted_quad111_y_values_tsr)\n",
    "\n",
    "# plot y-line prediction\n",
    "\n",
    "plt.plot(actual_x_values_tsr, predicted_quad111_y_values_tsr, 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact(a=1,b=1,c=1)\n",
    "def plot_both(a,b,c):\n",
    "    # interactive_predicted_quad_y_values_tsr = custom_quad_fn(actual_x_values_tsr)\n",
    "\n",
    "    plt.scatter(actual_x_values_tsr, actual_y_values_realistic_tsr)\n",
    "\n",
    "    actual_x_values_for_plotting_tsr = torch.linspace(-2.1,2.1,100)[:,None]\n",
    "    custom_quad_fn = mk_quad_fn(a,b,c)\n",
    "    interactive_predicted_quad_y_values_tsr = custom_quad_fn(actual_x_values_for_plotting_tsr)\n",
    "    plt.ylim((-3,13))\n",
    "    plt.plot(actual_x_values_for_plotting_tsr, interactive_predicted_quad_y_values_tsr, 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f347031dbe44a7184565f5e30bc26bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.1, description='a', max=3.3000000000000003, min=-1.1), FloatSlider(v‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "from fastai.basics import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plt.rc('figure', dpi=90)\n",
    "\n",
    "def plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n",
    "    x = torch.linspace(min,max, 100)[:,None]\n",
    "    if ylim: plt.ylim(ylim)\n",
    "    plt.plot(x, f(x), color)\n",
    "    if title is not None: plt.title(title)\n",
    "\n",
    "def f(x): return 3*x**2 + 2*x + 1\n",
    "\n",
    "# plot_function(f, \"$3x^2 + 2x + 1$\")\n",
    "\n",
    "def quad(a, b, c, x): return a*x**2 + b*x + c\n",
    "\n",
    "def mk_quad(a,b,c): return partial(quad, a,b,c)\n",
    "\n",
    "\n",
    "# f2 = mk_quad(3,2,1)\n",
    "# plot_function(f2)\n",
    "\n",
    "def noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\n",
    "\n",
    "def add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x = torch.linspace(-2, 2, steps=20)[:,None]\n",
    "y = add_noise(f(x), 0.15, 1.5)\n",
    "@interact(a=1.1, b=1.1, c=1.1)\n",
    "def plot_quad(a, b, c):\n",
    "    plt.scatter(x,y)\n",
    "    plot_function(mk_quad(a,b,c), ylim=(-3,13))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectified_linear(m,b,x):\n",
    "    y = m*x+b\n",
    "    return torch.clip(y, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function(partial(rectified_linear, 1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def rectified_linear2(m,b,x): return F.relu(m*x+b)\n",
    "plot_function(partial(rectified_linear2, 1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(m=1.5, b=1.5)\n",
    "def plot_relu(m, b):\n",
    "    plot_function(partial(rectified_linear, m,b), ylim=(-1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_relu(m1,b1,m2,b2,x):\n",
    "    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n",
    "\n",
    "@interact(m1=-1.5, b1=-1.5, m2=1.5, b2=1.5)\n",
    "def plot_double_relu(m1, b1, m2, b2):\n",
    "    plot_function(partial(double_relu, m1,b1,m2,b2), ylim=(-1,6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
