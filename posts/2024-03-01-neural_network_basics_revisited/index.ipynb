{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Neural Networks Basics (Revisited) - [DRAFT TBC 02/3/24]\"\n",
    "author: \"Tony Phung\"\n",
    "date: \"2024-03-04\"\n",
    "categories: [deep dive, laymens]\n",
    "image: \"deep.jpg\"\n",
    "toc: true\n",
    "description: \"Deeper dive into Neural Network basics\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](deep.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to dive into previously explored topics a bit more **deeply**. Time to try to understand what's actually going on.\n",
    "\n",
    "These posts help me trigger what kind of thought process I had at the time of learning the topic. It's **not be easily disgestable for other people ðŸ—ºï¸** but feel free to read on.\n",
    "\n",
    "Up til now, I've sped-run topics whilst not understanding much of the  details. \n",
    "\n",
    "I like to get the code running at least and produce results to expectation before getting in the weeds. At least I know the code works so I don't end up heading towards doom ðŸŒš. \n",
    "\n",
    "In these deeper dives:    \n",
    "\n",
    "- Lets hope to do these concepts justice (I'll try to get feedback from professionals at some point). Feel free to correct my understanding! (I'm sure some parts aren't on the ball ðŸ‘ŽðŸ™).  \n",
    "\n",
    "- The concepts and math are brand new to me (or done a decade ago), these posts help me express things in my way, which is usually the very lay way. It is a pretty raw thoughts at each step of the concepts and ideas I'm trying to understand; Con-steps? Step-deas? (Sorry â„ï¸).  \n",
    "\n",
    "- Also, without writing them down, I'll end up forgetting most of how and what I thought to conceive these concepts (I need to improve my vocabulary), i.e. it will be like these thoughts had never had happened (loss and irretrievable from the black-hole that is my mind âš«).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction  \n",
    "## 1.1 The Mission\n",
    "\n",
    "![](mission.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Mission]**:  \n",
    "\n",
    "- Create a `model` that takes in some `inputs` and provides a reasonble `output` (prediction).\n",
    "\n",
    "**[Method]**:  \n",
    "\n",
    "\n",
    "- A `mathematical function` is a form of a model:\n",
    "    - takes in inputs `x1, x2, ...` and \n",
    "    - `transforms` it into a single output `y = F(x1, x2, ...)`. \n",
    "\n",
    "This seems like a good appraoch to tackle the mission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 The Mathematical Model\n",
    "`Create a Linear Model` (or mathematical function) given some `(real world) data` (data that represents something we want to predict given similar input) in the form $$y = a_1x_1 + a_2x_2 + ... a_nx_n$$\n",
    "It is called `Linear` because our inputs `x` are of degree 1.  \n",
    "\n",
    "- Each `inputs` (`x1, x2, ..., xn`) is multiplied by their corresponding `coefficients` or `parameters` (`a1, a2, ..., an`), i.e.: $$a_1x_1 + a_2x_2 + ...$$ \n",
    "    - The `inputs` variables `x`'s are like features or characteristics of our model.\n",
    "    - the `coefficients` (known as `parameters`) in machine learning talk, scale the features/inputs. By scaling the inputs, it's like finding out which input/feature matters more or not, in determining the output. \n",
    "- The output `y` (known as `predictions`) is calculated by `summing all the scaled parameters` together: `a1x1 + a2x2 + ...`: $$F(x) = y = a_1x_1 + a_2x_2 + ... + a_nx_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Laymens (mathematical model):  \n",
    "- If a coefficient $a_1, a_2 ...$ is **almost zero**, it's like saying it does not impact the prediction value (remember we are adding up scaled versions of inputs to calculate the output / prediction). Then thats like saying perhaps this particualr parameter is an unimportant variable (or feature)! \n",
    "    - Perhaps we can get rid of it altogether? \n",
    "    - Less variables, less calculations, less overhead (and less work!) and a more simple model without losing significant predictive power.\n",
    "- If it has a large coefficient, then it would impact the overall sum, hence prediction. Probably shouldnt disregard this parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Mathematical models examples\n",
    "Two simple examples (single input `x` and a single output `y`) can be visualised on the `2D-plane`:\n",
    "\n",
    "- A `straight line` where input `x` is the horizontal-axis and output: `F(x)` and `y` is on the vertical-axis is with the formula we all know and love: $$ F(x) = mx + b $$\n",
    "\n",
    "And similarly,  \n",
    "\n",
    "- A `quadratic line` is: $$F(x) = ax^2 + bx + c$$\n",
    "\n",
    "**Note**: ***Our model should `generalise a mathematical function` (or find out the core characteristics and relationships ) given our set of data, so once we know that, we can predict unseen data with similar traits.***\n",
    "\n",
    "- For a quadratic equation, what combination of $a$, $b$ and $c$ will help us predict closely the value we want given some random $x$\n",
    "\n",
    "- How about all  of $x$ as we want and have good predictions all of them on average $x$? That's what we are trying to do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preparation\n",
    "\n",
    "## 2.1 Choose a Model  \n",
    "\n",
    "Why are we talking about models where the title is `Data Preparation`? \n",
    "\n",
    "As seen in our `Mission` statement, we wish to create a model to solve problems. In my mind, this means we should have some mental model of what we are trying to solve, and then we decide what data that goes into the model.\n",
    "\n",
    "If we just have some data without a sense of how it fits into a particular model in mind, how will know what to do with data, whether the data is relevant, its just a piece of data.\n",
    "\n",
    "Once we established our first model (we could change it later) then we can process our data to enter the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Quadratic Equation Example\n",
    "- Assume a quadratic equation: `F(x) = ax^2 + bx + c` can help us model some real world phenomena (e.g. throwing a ball or driving then stopping).  \n",
    "\n",
    "![](quad_real_world.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Make Predictions with our Model\n",
    "\n",
    "Great! We have our model, lets start making predictions! \n",
    "\n",
    "Not quite. There are alot of missing components that has not been decided in our equation.\n",
    "\n",
    "Also ***what does it mean to even make a prediction and how do we do it***?\n",
    "\n",
    "- In our `mathematical model` context, it's as simple as plugging in the different input values (`x`) and their coefficients (`a,b,c`) to our quadaratic equation (`F(x) = ax^2 + bx + c`) to see what output value (`y`) we get.    \n",
    "\n",
    "- In other words, calculate the `F(x)` (predictions) by using different combinations of our inputs `x` and parameters `a,b,c` with the equation `F(x) = ax^2 + bx + c`. Simple!\n",
    "\n",
    "But what combinations `a,b,c and x` do we to use? \n",
    "- Surely not random? Well kind of yes but also no.\n",
    "\n",
    "Note: input variables are known as `features` and coefficients are known as `parameters`. So, I'll use them interchangeably.\n",
    "\n",
    "***[QUESTION]***: \n",
    "- To make useful and relevant predictions, is there some **systematic way** to decide what ***starting*** ***`parameters`($a, b, c$)*** and ***`features`($x_1, x_2$)*** to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 ***Starting*** Inputs Variables $x_1, x_2, ... , x_n$   \n",
    "The question is $x$'s do we start with? \n",
    "\n",
    "Lets discuss a model that is a little more complex than a line on a 2-D plane.\n",
    "\n",
    "Say we want a predictive model that guesses whether a passenger on the Titanic:\n",
    "- **Survived** and \n",
    "- **Did not Survive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Good Starting Inputs Variables\n",
    "-  A `good set` of input variables would be data on actual passengers of the titanic and information on whether they survived or not.\n",
    "-  The learning model would learn about all the characteristics of these passengers and then adjust the parameters and scale towards characteristics that matter and and vice version on passengers and their survivability.\n",
    "- Through the training process, the model may find some characteristics to be irrelevant (perhaps ear length), but that is not the focus of this part of the process. This part of the process is where to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Bad Starting Inputs\n",
    "An `even worse` set of inputs (moving towards the bad `validation set` territory, more on this later). Data from unrelated sources (sorry to state the obvious) such as:   \n",
    "- the cohort of 2015 Applied Finance Graduates from Macquarie University, Sydney,  \n",
    "- a group people picked at random off streets of Binh Thanh Saigon, Vietnam,  \n",
    "- a group of online matches from a popular dating app obtained by a user.  \n",
    "These groups  technically surivived the Titanic so the model will say all their characteristics of everyone are useful, which isnt a great predictor (it'll predict being born after the Titanic sunk gives you a 100% chance of surviving the Titanic ðŸ¤­)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 ***Starting*** Coefficients $x_1, x_2, ... , x_n$   \n",
    "\n",
    "I heard determeting starting coefficients is ***more of an art than a science*** (Lets assume later there are some cool algorithms that give us a hand in the future)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 About Zero\n",
    "\n",
    "It would make sense that initially **input dominate parameters** (input values are large relative to their coefficients) and then they're scaled by a **parameter** according to importance through the neural network.  \n",
    "- This means, start with **relatively low value parameters** (to it's input variables) and let these coefficients adjust (`learn` through `gradient descent`) to larger values, whether positive or negative.  \n",
    "- In other words start with coefficients **about zero**.\n",
    "\n",
    "### 2.4.1.1 Example  \n",
    "- Say we play a game where I tell you to choose a starting value and take count by 1 towards my guess and you want to choose a starting value to minimise the count on average.   \n",
    "- And I tell you my guess is always random and between `-50 and 50`.   \n",
    "- You'd probably pick zero because its common sense! (I hope it is)  \n",
    "- It's kind of same in our model, except we dont pick zero itself because it ruins the math (explain later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Titanic\n",
    "Lets assume we have a parameters should be relatively large and positive (for e.g. being a female or child in first class on the Titanic is probably two good parameters for survivability).  \n",
    "- If the parameter starts off large and negative then this causes the model to take alot of iterations to learn and adjust the parameter to be positive. And vice versa.  \n",
    "- So if we set all near zero, it will simply take `less time on average` for most models for each parameter to get to whether they need to go (`optimisation?`) considering we don't know what parameters matter or not, so its random for us, ie, it can be positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Preset Coefficients and Pre-trained Models\n",
    "`Setting some coefficients initially` (rather than using all random coefficients) and then training the model to the training set, this is actually called `learning with a pre-trained model` (not set by us). \n",
    "- This pre-trained model has already been run and completed by someone else and we are teaching the model learn about our data on top of its knowledge.  \n",
    "- Most of what I'll be initially doing is `Building a Learner from a Pre-trained Model`. In the future, I hope to learn to build the Pre-Trained models from scratch, if its a useful endeavour?     \n",
    "- Popular examples of pre-trained models are `ResNet50`, which are trained on (millions of) pictures with 50 layers deep neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 Isn't it all relative?\n",
    "\n",
    "Doesn't a coefficients **relativeness** depend on the actual value of its **corresponding input parameter**. \n",
    "- So shouldn't the starting coefficient be set **relative to its corresponding starting input parameter**, and \n",
    "- Not just an `arbitrary range of -0.5 to 0.5`?\n",
    "\n",
    "Also, input variables can **range drastically** **within itself** and **relative to other variables**! For example:\n",
    "- `age` could be range `from 1 to 130` years old   \n",
    "- `Amount of money fundraised or donated` by people could range `0 to billions`  \n",
    "- `Lenght of the petals of a plum flower` could range `0.1 mm to 40 mm`.  \n",
    "- `x-values` of any line function could be negative $\\infty$ to positive $\\infty$ (ngl I wanted to use the $\\infty$ sign for once in my life â™¾ï¸)\n",
    "\n",
    "With all these unaswered questions...\n",
    "\n",
    "***Does it still make sense to just choose coefficients about zero? (-0.5 to 0.5)?***  \n",
    "\n",
    "Yes, because of normalisation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalisation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Data normalization` (in ML) refers to the process of `scaling` and `standardizing` the features of a dataset.\n",
    "- This ensures all `features contribute equally to the learning process` \n",
    "- To avoid issues related to `different scales and units` in the input data. \n",
    "\n",
    "Are there different techniques for `numeric` versis `categorical` data? Absolutely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Numerical Data Techniques:\n",
    "***Min-Max Scaling***:\n",
    "- Scales the data to a specific range, often [0, 1].\n",
    "\n",
    "***Standardization (Z-score normalization)***:\n",
    "- Scales the data to have a mean of 0 and a standard deviation of 1.\n",
    "    \n",
    "***Robust Scaling***:\n",
    "- Uses the median and interquartile range (IQR) instead of the mean and standard deviation (Similar to standardization). Robust Scaling is less sensitive to outliers than standardization.\n",
    "\n",
    "***Log Transformation***:\n",
    "- Applied when the data is skewed, and you want to reduce the impact of large values.\n",
    "\n",
    "***Normalization in Neural Networks***:\n",
    "- Normalize input features to zero mean and unit variance. Batch normalization layers can also be used within the network to normalize the activations of hidden layers during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Problem with Normalised Data\n",
    "\n",
    "During gradient descent, the algorithm updates the model weights based on the direction of the steepest descent in the cost function (error). \n",
    "\n",
    "For simple linear regression model to predict housing prices based on two features: square footage (ftÂ²) and number of bedrooms: \n",
    "- the large values of square footage will have a `much stronger influence` on the `gradient calculation` compared to the number of bedrooms.\n",
    "- This can lead the algorithm to prioritize changes based on square footage, potentially neglecting the impact of bedrooms on price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Benefits to Gradient Descent\n",
    "Gradient descent is an iterative optimization algorithm commonly used in machine learning for training models. It aims to find the minimum of a cost function by adjusting model parameters iteratively.\n",
    "\n",
    "***Gradient Descent Sensitivity to Scale***:\n",
    "- The scale of the input features directly influences the scale of the gradients. Features with larger scales can dominate the learning process and cause the algorithm to converge slowly or not converge at all.\n",
    "\n",
    "***Equalizing the Influence of Features***:\n",
    "- Normalizing data ensures that all features have a similar scale, `preventing the algorithm from being biased` towards features with larger magnitudes.\n",
    "- This equalization is particularly important when features have different units or scales, as it allows each feature to contribute proportionally to the model's parameter updates during training.\n",
    "\n",
    "***Faster Convergence***:\n",
    "- Normalization often leads to `faster convergence` during training. With well-scaled features, the algorithm can take more substantial steps towards the minimum of the cost function, `reducing the number of iterations` needed for convergence.\n",
    "\n",
    "***Improved Numerical Stability***:\n",
    "- Normalization enhances the numerical stability of the optimization process. It helps prevent issues like `vanishing or exploding gradients`, which can occur when working with features of vastly different magnitudes.\n",
    "\n",
    "***Regularization Considerations***:\n",
    "- Some regularization techniques, such as L1 and L2 regularization, are sensitive to the scale of features. Normalizing features ensures that regularization terms act uniformly across all features, leading to a fair and effective regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Categorial Data Techniques:\n",
    "Categorical data itself cannot be normalized in the same way numerical data is\n",
    "\n",
    "***Dummy Variables or (One-Hot Encoding)***: \n",
    "- Create a new binary feature for each category within the original categorical variable. \n",
    "- Each new feature gets a value of 1 if the data point belongs to that category, and 0 otherwise.\n",
    "- This is a more robust approach.\n",
    "\n",
    "***Label Encoding***: \n",
    "- Assign a unique numerical value (integer) to each category. \n",
    "    - For example, \"Blue\" = 1, \"Brown\" = 2, and \"Green\" = 3 for eye color categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model\n",
    "\n",
    "Create data based on the a quadratic equation and add some noise to it simulate real world\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Create the Quadratic Equation\n",
    "\n",
    "From the general equation $$ F(a,b,c,x) = ax^2 + b + c $$\n",
    "Simulate data from $$ F(3,2,1,x) = 3x^2 + 2x + 1 $$\n",
    "where $a = 3, b = 2, c = 1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def quad_fn(a,b,c,x): return a*x**2 + b*x + c #\n",
    "def custom_quad_fn(a,b,c): return partial(quad_fn, a,b,c) # partial fixes a b c\n",
    "\n",
    "quad_og_fn = custom_quad_fn(3,2,1) # og function\n",
    "\n",
    "# quad_111(1) # quadfn f(1): 1(1)^2 + 1(1) + 1 = 1 + 1 + 1 = 3 \n",
    "# quad_222(2) # quadfn f(2): 1(2)^2 + 1(2) + 1 = 4 + 2 + 1 = 7\n",
    "# quad_333(3) # quadfn f(3): 1(3)^2 + 1(3) + 1 = 9 + 3 + 1 = 13 \n",
    "\n",
    "quad_og_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Quadratic Function Plotter\n",
    "***Inputs***: \n",
    "- custom_quad_fn(a,b,c) - *[partial_fn]* \n",
    "- x-limits (min, max) - [*numeric*] \n",
    "- colour [*str*]\n",
    "- y-limit -[*numeric*]  \n",
    "\n",
    "***Outputs***:\n",
    "- `x-values - [tensor]`: creates 2-D tensor of x-values (not sure why we need a 2D yet) but its done by applying `[:,None]` to a 1D-Tensor\n",
    "- `f(x) - [tensor]`: output of custom_quad_fn based on x-values (tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import torch\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(a=1.1,b=1.1,c=1.1)\n",
    "def quad_plotter(a,b,c):\n",
    "    quad_abc_fn = custom_quad_fn(a,b,c)    # 1. create quad abc\n",
    "    xs = torch.linspace(-2.1, 2.1, steps=100)     # 2. create xs\n",
    "    ys = quad_abc_fn(xs)   # 3. create ys\n",
    "    plt.ylim((-3,15))# 4. set limits\n",
    "    plt.xlim((-2,2))\n",
    "    plt.plot(xs, ys, color='r')    #5. plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Synthetic Data\n",
    "\n",
    "Lets create some Gaussian Noise and add it out Quadratic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(input_tnsr_1D, std_dev):# Creates an N(0,std_dev) of same shape as input_tnsr\n",
    "    np.random.seed(42)     \n",
    "    return torch.tensor(np.random.normal(scale=std_dev, size = input_tnsr_1D.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Noise\n",
    "\n",
    "So why is only `scaling` our data by this `gaussian noise` not enough? That is, why do you need to have an `addititive` component?\n",
    "\n",
    "- By definition, large input values are always relatively larger than smaller input values, thus our edges of the original quadratic function (values further away from zero), will have greater variation than those about zero. By multiplying evey point by random guassian scaler, the edges will be larger on avearge.\n",
    "    - This means there is unneccesary larger descrepancies about our original function due to the nature of our function, and that isnt what we are after.\n",
    "    - We want random data distrbuted normally about our original data, regardless of the value.\n",
    "    - It makes most sense to have a relatively smaller scaling/multiplicative factor and have some uniform randomly distributed `additive` component.\n",
    "\n",
    "Controlling the Impact on Signal Strength:\n",
    "\n",
    "***Additive noise***: This can overwhelm weaker signals by adding a constant amount of noise, making them difficult to recover or analyze.\n",
    "\n",
    "***Multiplicative noise***: This can preserve the relative strengths of different signals in the data. Even if a weak signal is amplified or attenuated by a small factor due to the noise, it remains proportionally weaker compared to stronger signals. This can be crucial for tasks like signal separation or classification where maintaining relative signal strengths is important.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Sample of Original Quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_og_10_vals_tnsr = torch.linspace(-2, 2, steps = 20)\n",
    "ys_og_10_vals_tnsr = quad_og_fn(xs_og_10_vals_tnsr) # 10 og data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Add Gaussian Noise to Sample of Original Quadratic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 og + noise data points\n",
    "# 1. get [10_og_data_points]\n",
    "ys_og_10_vals_tnsr\n",
    "# 2a. get random scaler [10_vals_tnrs]:\n",
    "noise_scaler_10_vals_tnsr = noise(ys_og_10_vals_tnsr, 0.15)\n",
    "noise_adder_10_vals_tnsr = noise(ys_og_10_vals_tnsr, 1.5)\n",
    "# noise_scaler_10_vals_tnsr\n",
    "# 2b. apply random scaler to og data\n",
    "scalars_10_vals_tnsr = noise_scaler_10_vals_tnsr * ys_og_10_vals_tnsr \n",
    "ys_scaled_og_10_vals_tnsr = ys_og_10_vals_tnsr + scalars_10_vals_tnsr\n",
    "# 3a. get random_additive [10_vals_tnrs]: \n",
    "\n",
    "ys_syn_data_og_n_noise_10_vals_tnsr = ys_scaled_og_10_vals_tnsr + noise_adder_10_vals_tnsr \n",
    "# 4. result: this is [synthetic_10_data_points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Plot Quad and Noisey Quad\n",
    "### 5.4.1 Dirty Version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 og + noise data points\n",
    "# 1. get [10_og_data_points]\n",
    "ys_og_10_vals_tnsr\n",
    "# 2a. get random scaler [10_vals_tnrs]:\n",
    "noise_scaler_10_vals_tnsr = noise(ys_og_10_vals_tnsr, 0.15)\n",
    "noise_adder_10_vals_tnsr = noise(ys_og_10_vals_tnsr, 1.5)\n",
    "# noise_scaler_10_vals_tnsr\n",
    "# 2b. apply random scaler to og data\n",
    "scalars_10_vals_tnsr = noise_scaler_10_vals_tnsr * ys_og_10_vals_tnsr \n",
    "ys_scaled_og_10_vals_tnsr = ys_og_10_vals_tnsr + scalars_10_vals_tnsr\n",
    "# 3a. get random_additive [10_vals_tnrs]: \n",
    "\n",
    "ys_syn_data_og_n_noise_10_vals_tnsr = ys_scaled_og_10_vals_tnsr + noise_adder_10_vals_tnsr \n",
    "# 4. result: this is [synthetic_10_data_points]\n",
    "\n",
    "\n",
    "# from ipywidgets import interact\n",
    "# @interact(a=1.1,b=1.1,c=1.1)\n",
    "plt.scatter(xs_og_10_vals_tnsr, ys_syn_data_og_n_noise_10_vals_tnsr)\n",
    "\n",
    "a=1.1\n",
    "b=1.1\n",
    "c=1.1\n",
    "quad_plotter(a,b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 Clean Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(a=1.1,b=1.1,c=1.1)\n",
    "def quad_scatter_plotter(a,b,c):\n",
    "    plt.scatter(xs_og_10_vals_tnsr, ys_syn_data_og_n_noise_10_vals_tnsr)\n",
    "    quad_plotter(a,b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Metrics\n",
    "\n",
    "It'll be great if we can have some metrix to measure how close our plot is to the noisey data.\n",
    "\n",
    "By measuring the distance between our prediction (red line) and the data (red dots), and sum all the differences then we have some measure of our accuracy. There are two popular metrics in Machine Learning:  \n",
    "- Mean Absolute Error (MAE) \n",
    "- Mean Squared Error (MSE).\n",
    "\n",
    "**Note**: We didn't simply take the differences because our predictions ***will cancel out*** as our the data lies above and below our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Mean Absolute Error (MAE)\n",
    " \n",
    "1. Calculate the Differences between each Prediction and the Data point\n",
    "2. Apply Absolute function (or square if we're doing MSE)\n",
    "3. Take the Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1.1 Dirty Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. get 10 synthethic data points\n",
    "ys_syn_data_og_n_noise_10_vals_tnsr\n",
    "# 2. get 10 predictions\n",
    "a=1.1\n",
    "b=1.1\n",
    "c=1.1\n",
    "curr_quad_abc_predictive_model = custom_quad_fn(a,b,c)\n",
    "\n",
    "ys_curr_quad_predictions_tnsr = curr_quad_abc_predictive_model(xs_og_10_vals_tnsr)\n",
    "diff_preds_vs_syn_data_tnsr = ys_curr_quad_predictions_tnsr-ys_syn_data_og_n_noise_10_vals_tnsr\n",
    "abs_diff_preds_vs_syn_data_tnsr =torch.abs(diff_preds_vs_syn_data_tnsr)\n",
    "mae_calc = abs_diff_preds_vs_syn_data_tnsr.mean()\n",
    "mae_calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1.2 Clean Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_quad_abc_predictive_model = custom_quad_fn(a,b,c)\n",
    "ys_curr_quad_predictions_tnsr = curr_quad_abc_predictive_model(xs_og_10_vals_tnsr)\n",
    "\n",
    "def custom_mae(preds, actual):\n",
    "    diff_preds_vs_syn_data_tnsr = preds-actual\n",
    "    abs_diff_preds_vs_syn_data_tnsr =torch.abs(diff_preds_vs_syn_data_tnsr)\n",
    "    loss = abs_diff_preds_vs_syn_data_tnsr.mean()\n",
    "    return loss\n",
    "\n",
    "custom_mae(ys_curr_quad_predictions_tnsr,ys_syn_data_og_n_noise_10_vals_tnsr)\n",
    "#  3. mae \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Plot with MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(a=1.1,b=1.1,c=1.1)\n",
    "def quad_scatter_plotter(a,b,c):\n",
    "    curr_quad_abc_predictive_model = custom_quad_fn(a,b,c)\n",
    "    ys_curr_quad_predictions_tnsr = curr_quad_abc_predictive_model(xs_og_10_vals_tnsr)\n",
    "    loss = custom_mae(ys_curr_quad_predictions_tnsr,ys_syn_data_og_n_noise_10_vals_tnsr)\n",
    "\n",
    "    plt.title(f\"mae: {loss:.2f}\")\n",
    "    plt.scatter(xs_og_10_vals_tnsr, ys_syn_data_og_n_noise_10_vals_tnsr)\n",
    "    quad_plotter(a,b,c)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 MAE Calculator\n",
    "\n",
    "Create a function that accepts parameters (of a quadratic) and calculates the MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_mae(param_tnsr):\n",
    "    pred_model = custom_quad_fn(*param_tnsr)\n",
    "    preds = pred_model(xs_og_10_vals_tnsr)\n",
    "    loss = custom_mae(preds,ys_syn_data_og_n_noise_10_vals_tnsr)\n",
    "    return loss\n",
    "\n",
    "a=1.1\n",
    "b=1.1\n",
    "c=1.1\n",
    "abc_tsr = torch.tensor([a,b,c])\n",
    "abc_tsr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Loss Function without `requires_grad_()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abc_tnsr\n",
    "# loss_function_mae([1.1,1.1,1.1])\n",
    "loss_function_mae(abc_tsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Apply `requires_grad_()` method to input tensor\n",
    "\n",
    "By calling this method on this tensor, each time this tensor is used in function, the gradients are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_tsr.requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Loss Function with `requires_grad_()`\n",
    "\n",
    "`abc_tnsr` has been called and its gradients can be calculuated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_function_mae(abc_tsr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Apply `backward()` to a Loss function to calculate gradients of the inputs parameters\n",
    "\n",
    "`.backward()` is called on  and its attrbiute `.grad` is populated with gradients related to the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_tsr.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "# 1. get tensor abc_tnsr\n",
    "# 2. requires_grad_()\n",
    "\n",
    "# loop\n",
    "# 1. with torch.no_grad():  [a,b,c]\n",
    "# 2. calc gradients:        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    abc_tsr -= abc_tsr.grad*0.01\n",
    "    loss = loss_function_mae(abc_tsr)\n",
    "\n",
    "loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    with torch.no_grad():\n",
    "        abc_tnsr -= abc_tnsr.grad*0.01\n",
    "        loss = loss_function_mae(abc_tnsr)\n",
    "        print(f\"{loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.4 TBA STEP UPDATE PARAMETERS\n",
    "- `updating parameters` to \n",
    "    - `improve accuracy` of our predictions (i.e. decrease difference between our predictions and data)\n",
    "    - `do it automatically`: the art of improving our model or `learning` is called `gradient descent`.  \n",
    "- `neural network`: Once the model is accuracy we are happy with, this is our `neural network`.\n",
    "\n",
    "Quite simple really?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulate Data\n",
    "Simulate content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch\n",
    "\n",
    "np.random.seed(42)\n",
    "def noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\n",
    "def add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n",
    "\n",
    "actual_x_values_tsr = torch.linspace(-2, 2, steps=20)[:,None] # simulate 20 actual x-values + shape to 2D tensor\n",
    "\n",
    "def actual_function(x): return 3*x**2 + 2*x + 1 \n",
    "# In reality we don't have a real function like this to use, \n",
    "# however we use this + add noise, which simulates real data\n",
    "# then we try to model this noisey data\n",
    "actual_y_values_unrealistic_tsr = actual_function(actual_x_values_tsr)\n",
    " # actual y-values of funtion we use to find\n",
    "\n",
    "#  but again, these values are not realistic because they're based on the real function - something that doenst exist in real life,\n",
    "# its like having the exact function that determines whether a photo is a cat or not\n",
    "# we can only approximate functions and its parameters\n",
    "# in real world, data has noise,\n",
    "# introduce data to these unrealistic real values to product realistic actual values\n",
    "\n",
    "# okay so why dont we need to add noise to actual_x_values? its any input is realistic/real world\n",
    "# any photo is can be asked 'is it a cat?'\n",
    "# any passenger with any characters can be asked 'did the passenger survive?\n",
    "\n",
    "actual_y_values_realistic_tsr = add_noise(actual_y_values_unrealistic_tsr, 0.15, 1.5) # use actualy y-values + add noise - to create simulated real data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we have a set of (actual realistic) 'data'\n",
    "# 1. try 'model' it with a quadratic equation \n",
    "# 2. create loss function - mean absolute error - difference between each point of actual y vs predicted y at each x, find difference and then absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_x_values[:5],actual_y_values_realistic[:5]\n",
    "\n",
    "# 1. Create quad function with parameters\n",
    "def quad_fn(a,b,c,x): return a*x**2 + b*x + c\n",
    "y_a1_b1_c1_x1 = quad_fn(1,1,1,1)\n",
    "y_a1_b1_c1_x1  # 3 = (1*1^2) + (1*1) + (1) = 1+1+1\n",
    "y_a1_b1_c1_x2 = quad_fn(1,1,1,2)\n",
    "y_a1_b1_c1_x2  # 7 = (1*2^2) + (2*1) + (2) = 4+2+1\n",
    "\n",
    "# Its quite cumbersome to put a new x-value through the function, to get a corresponding predicted y-value\n",
    "# ideally we can provide a list of xs to get a list of predicted ys (x-tensor -> f -> y-tensor)\n",
    "# and also the coefficients are parameterised\n",
    "from functools import partial\n",
    "def mk_quad_fn(a,b,c): return partial(quad_fn,a,b,c)\n",
    "quad111 = mk_quad_fn(1,1,1)\n",
    "# quad111(actual_x_values)\n",
    "predicted_quad111_y_values_tsr = quad111(actual_x_values_tsr) # remember is a 2d tensor due to added dimension with: [:,None]\n",
    "\n",
    "# we have actual-yand predicted-y data, lets compare them\n",
    "\n",
    "def mae_calc(actual, preds): return abs(preds-actual).mean()\n",
    "\n",
    "mae_calc(actual_y_values_realistic_tsr,predicted_quad111_y_values_tsr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.plot(actual_y_values_realistic_tsr,predicted_quad111_y_values_tsr)\n",
    "plt.scatter(actual_x_values_tsr, actual_y_values_realistic_tsr)\n",
    "\n",
    "# plot predictions\n",
    "# for predictions, graphically it will look better to plot a line \n",
    "# rather than just a coressponding y-prediction to each actual y\n",
    "\n",
    "# lets do just corresponding ones first to see what it looks like\n",
    "\n",
    "plt.scatter(actual_x_values_tsr, predicted_quad111_y_values_tsr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(actual_x_values_tsr, actual_y_values_realistic_tsr)\n",
    "# plt.scatter(actual_x_values_tsr, predicted_quad111_y_values_tsr)\n",
    "\n",
    "# plot y-line prediction\n",
    "\n",
    "plt.plot(actual_x_values_tsr, predicted_quad111_y_values_tsr, 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact(a=1,b=1,c=1)\n",
    "def plot_both(a,b,c):\n",
    "    # interactive_predicted_quad_y_values_tsr = custom_quad_fn(actual_x_values_tsr)\n",
    "\n",
    "    plt.scatter(actual_x_values_tsr, actual_y_values_realistic_tsr)\n",
    "\n",
    "    actual_x_values_for_plotting_tsr = torch.linspace(-2.1,2.1,100)[:,None]\n",
    "    custom_quad_fn = mk_quad_fn(a,b,c)\n",
    "    interactive_predicted_quad_y_values_tsr = custom_quad_fn(actual_x_values_for_plotting_tsr)\n",
    "    plt.ylim((-3,13))\n",
    "    plt.plot(actual_x_values_for_plotting_tsr, interactive_predicted_quad_y_values_tsr, 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import torch\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "# 1. create synthetic data\n",
    "\n",
    "# 1.1 create base data\n",
    "def quad(a,b,c,x): return a*x**2 + b*x + c\n",
    "def partial_quad_abc(a,b,c): return partial(quad,a,b,c)\n",
    "quad_321_fn = partial_quad_abc(3,2,1)\n",
    "\n",
    "@interact(a=3,b=2,c=1)\n",
    "def func_plotter(a,b,c):\n",
    "    xs_input_tnsr = torch.linspace(-2.1,2.1, steps=100)\n",
    "    pred_model = partial_quad_abc(a,b,c)\n",
    "    ys_preds_tnsr = pred_model(xs_input_tnsr)\n",
    "\n",
    "    plt.xlim((-2,2))\n",
    "    plt.ylim((-1,15))\n",
    "    plt.plot(xs_input_tnsr,ys_preds_tnsr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e653c662fb46219305d049dad586c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='a', max=9, min=-3), IntSlider(value=2, description='b', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "import torch\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1 create model data\n",
    "def quad(a,b,c,x): return a*x**2 + b*x + c\n",
    "def partial_quad_abc(a,b,c): return partial(quad,a,b,c)\n",
    "quad_321_fn = partial_quad_abc(3,2,1)\n",
    "\n",
    "\n",
    "@interact(a=3,b=2,c=1)\n",
    "def func_plotter(a,b,c):\n",
    "    xs_input_tnsr = torch.linspace(-2.1,2.1, steps=100)\n",
    "    pred_model = partial_quad_abc(a,b,c)\n",
    "    ys_preds_tnsr = pred_model(xs_input_tnsr)\n",
    "    plt.xlim((-2.1,2.1))\n",
    "    plt.ylim((-1,15))\n",
    "    plt.plot(xs_input_tnsr,ys_preds_tnsr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f20ccc1fdec4c58ba6af1f1448a6073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='a', max=9, min=-3), IntSlider(value=2, description='b', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. create syn data\n",
    "# 2.1 noise\n",
    "def noise(tsr, scale): return np.random.normal(scale=scale, size=tsr.shape)\n",
    "\n",
    "def add_scale_noise(tsr, add, scale):\n",
    "    scaled_noise = noise(tsr, scale)\n",
    "    adding_noise = noise(tsr, add)\n",
    "    return tsr*(1+scaled_noise) + adding_noise\n",
    "    # return tsr*(1+noise(tsr, scale)) + noise(tsr, add)\n",
    "\n",
    "\n",
    "xs_actual_inputs = xs_20samples_321_tsr = torch.linspace(-2,2,steps=20)\n",
    "ys_20samples_321_tsr = quad_321_fn(xs_20samples_321_tsr)\n",
    "\n",
    "np.random.seed(42)\n",
    "ys_actual = ys_actual_noisy_321_tsr = add_scale_noise(ys_20samples_321_tsr, 1.5, 0.15)\n",
    "# 2.1 scale and add\n",
    "\n",
    "def mae_calc(actual, preds): return torch.abs(preds-actual).mean()\n",
    "\n",
    "@interact(a=3,b=2,c=1)\n",
    "def plot_pred_and_actual(a,b,c):\n",
    "    func_plotter(a,b,c)\n",
    "    plt.scatter(xs_actual_inputs,ys_actual, color=\"r\")\n",
    "    mae = mae_calc(ys_actual,partial_quad_abc(a,b,c)(xs_actual_inputs))\n",
    "    plt.title(label=f\"MAE: {mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create function with \n",
    "# input (params or a tnsr) and \n",
    "# output (mae), hence apply gradient descent\n",
    "    \n",
    "def loss_function(params):\n",
    "    # mae = mae_calc(ys_actual,partial_quad_abc(a,b,c)(xs_actual_inputs))\n",
    "    ys_preds = partial_quad_abc(*params)(xs_actual_inputs)\n",
    "    mae = mae_calc(ys_actual,ys_preds)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. start with random param [abc_tsr] + [requires_grad]\n",
    "# 2. calc [loss]\n",
    "# 3. calc [gradients]\n",
    "# 4. do [gradient_descent]\n",
    "# 5. profit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4219, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. start with random param [abc_tsr] + [requires_grad]\n",
    "abc_tsr = torch.tensor([1.1,1.1,1.1], requires_grad=True)\n",
    "\n",
    "# 2. calc [loss]\n",
    "loss = loss_function(abc_tsr)\n",
    "loss \n",
    "\n",
    "# 3. calc [gradients]\n",
    "# loss.backward()\n",
    "# abc_tsr.grad\n",
    "# loss.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4219, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.3386, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.2345, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 4. do [gradient_descent]\n",
    "for i in range(3):\n",
    "    loss = loss_function(abc_tsr)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        abc_tsr -= abc_tsr.grad * 0.01\n",
    "        print(loss)\n",
    "        # print(abc_tsr.grad)\n",
    "        # print(abc_tsr)\n",
    "        # print(loss)\n",
    "        # print(abc_tsr.grad*0.01)\n",
    "    # 5. profit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abc_tnsr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12776/2685502710.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mabc_tnsr\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mabc_tnsr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{loss:.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'abc_tnsr' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    loss = loss_function(abc_tsr)  # Calculate loss outside computation graph\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        abc_tsr -= abc_tsr.grad * 0.01\n",
    "\n",
    "    print(f\"{loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
