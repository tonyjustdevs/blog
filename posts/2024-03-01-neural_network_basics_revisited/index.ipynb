{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Neural Networks Basics (revisited) - [DRAFT]\"\n",
    "author: \"Tony Phung\"\n",
    "date: \"2024-03-01\"\n",
    "categories: [deep learning, neural networks]\n",
    "image: \"deep.jpg\"\n",
    "toc: true\n",
    "description: \"Deeper dive into Neural Network basics\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](deep.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time for me to dive into previous important topics more deeply (pun unintended âœŠ). Time to try to understand what's actually going on.\n",
    "\n",
    "These posts are a way for me to trigger some memory on what I thought at the time of learning everything. Certainly it isnt **not be easily disgestable for other people ðŸ—ºï¸** but feel free to read on.\n",
    "\n",
    "Up to this point I've sped-run through the topics whilst not understanding the details. I try get the code running and produce results to expectation so at least I know it does work so I can come back later for the details. \n",
    "\n",
    "In these deeper dives:    \n",
    "\n",
    "- I hope to do these concepts justice (Will try to get feedback from professionals at some point). Feel free to correct my understanding if anything seems off (I'm sure some parts aren't on the ball ðŸ‘ŽðŸ™).  \n",
    "\n",
    "- The concepts and math are brand new to me (or done a decade ago), so these posts are a way for me to express them back to myself in a way I can understand. It is a pretty pure recollection of my thoughts at each step of the concept ideas. Con-steps? Step-deas? (Sorry â„ï¸).  \n",
    "\n",
    "- Also, I know full well that without writing how I conceived these concepts, it will be like these thoughts had never happened (loss and irretrievable from the black-hole that is my mind âš«).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction  \n",
    "### 1.1 The Mission\n",
    "\n",
    "![](mission.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Mission]**:   \n",
    "- Create a `model` that takes in some `inputs` and provides a reasonble `output`.  \n",
    "\n",
    "**[Method]**: \n",
    "- A `mathematical function` is a form of a model:\n",
    "    - takes in inputs `x1, x2, ...` and \n",
    "    - `transforms` it into a single output `y = F(x1, x2, ...)`. \n",
    "\n",
    "This seems like a good appraoch to tackle the mission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The Mathematical Model\n",
    "`Create a Linear Model` (or mathematical function) given some `(real world) data` (data that represents something we want to predict given similar input) in the form $$y = a_1x_1 + a_2x_2 + ... a_nx_n$$\n",
    "It is called `Linear` because our inputs `x` are of degree 1.  \n",
    "\n",
    "- Each `inputs` (`x1, x2, ..., xn`) is multiplied by their corresponding `coefficients` or `parameters` (`a1, a2, ..., an`), i.e.: $$a_1x_1 + a_2x_2 + ...$$ \n",
    "    - The `inputs` variables `x`'s are like features or characteristics of our model.\n",
    "    - the `coefficients` (known as `parameters`) in machine learning talk, scale the features/inputs. By scaling the inputs, it's like finding out which input/feature matters more or not, in determining the output. \n",
    "- The output `y` (known as `predictions`) is calculated by `summing all the scaled parameters` together: `a1x1 + a2x2 + ...`: $$F(x) = y = a_1x_1 + a_2x_2 + ... + a_nx_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Laymens (mathematical model):  \n",
    "- If a coefficient of a parameter is **almost zero**, it's like saying it does not impact the prediction value (remember we are adding up scaled versions of inputs to calculate the output / prediction). Then thats like saying perhaps this particualr parameter unimportant feature! Perhaps we can get rid of it altogether? Less variables, less calculations, less overhead, more simple more without losing significant accuracy.\n",
    "- If it has a large coefficient, then it would impact the overall sum, hence prediction. Probably shouldnt disregard this parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Simple Examples (of mathematical models)\n",
    "Two simple examples (single input `x` and a single output `y`) can be visualised on the `2D-plane`:\n",
    "\n",
    "- A `straight line` where input `x` is the horizontal-axis and output: `F(x)` and `y` is on the vertical-axis is with the formula we all know and love: $$ F(x) = mx + b $$\n",
    "\n",
    "And similarly,  \n",
    "\n",
    "- A `quadratic line` is: $$F(x) = ax^2 + bx + c$$\n",
    "\n",
    "**Note**: ***Our model should `generalise a mathematical function` (or find out the core characteristics and relationships ) given our set of data, so once we know that, we can predict unseen data with similar traits.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Steps  \n",
    "\n",
    "#### 1.4.1 Step 1: Create General Quadratic Equation/Model  \n",
    "\n",
    "Assume a quadratic equation: `F(x) = ax^2 + bx + c` can help us model some real world phenomena (e.g. throwing a ball or driving then stopping).  \n",
    "\n",
    "![](quad_real_world.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Step 2: Set Inputs\n",
    "$F(x)$: `make prediction` with `random coefficients`. \n",
    "\n",
    "***But what does making predictions mean*** in this context?\n",
    "\n",
    "It's simply plugging in the input values (`a,b,c,x`) to our equation to get the output value (`y`).   \n",
    "\n",
    "That is, calcualte the `F(x)` (predictions) by using different combinations of our inputs `x` and parameters (coefficients) `a,b,c` with the equation `F(x) = ax^2 + bx + c`.\n",
    "\n",
    "So in order to make predictions, we need to systematic way to decide what `coefficients` and `variables` to select.\n",
    "\n",
    "##### 1.4.2.1 Component 1: Parameters `a, b, and c` \n",
    "- **[Action]**: Set our parameters to be random numbers.\n",
    "\n",
    "##### 1.4.2.2 Component 2: Input `x` \n",
    "Set a range values input values. \n",
    "- **Note**: In real world and here there is always a set of real world data. \n",
    "- Unfortunately computers can really interpret photos or audio or text in its aesthestic form like humans. Fortunately, these different modes can be represented with numbers which computers are very good at handling.\n",
    "    - A **picture** are made up on pixels. Each pixel is a combination of a 3 parameters input (R,G,B) to create the colour.\n",
    "    - A piece of **Text** can be decomposed into words and letters and be labelled with integers too.\n",
    "For our model, given that we have some `real y-values` mapped to some `input x-values`, we should at least have predictions on those x-values to see how our predicted y-values are doing (known as `accuracy`, more later)\n",
    "iii. calculate each `F(x1), F(x2), etc `, by going through each combination `a,b,c and x`:\n",
    "    -`F(x1) = a1x2^2 + b1x1 + c1` (prediction 1 `y1 or F(x1) or F(x1,a1,b1,c1)` given this specific set of input variable `x1` and parameters `a1, b1, c1`)\n",
    "    -`F(x2) = a2x2^2 + b2x2 + c2`, (prediction 2)... etc  \n",
    "\n",
    "**Note**: \n",
    "- These coefficients are started as `random` because we have to start somewhere. \n",
    "- Starting off random parameters is generally a good practice, the art is then to incrementally updating our coefficients (known as `gradient descent`) to make our model (`neural network`) do better (`improving accuracy` or `decreasing our loss function`) at each iteration (known as `epoch`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Step 3: Create General Quadratic Equation/Model\n",
    "3. `measure accuracy` of our predictions i.e. `Mean Absolute Error (MAE)`. \n",
    "    - For each actual output/value `y`, what is our `predicted y`.\n",
    "    - Since predictions can be `above or below` the actual value, we apply an absolute function to measure the difference or `loss`. That is, the MAE is also known as our `Loss Function`. An alternate popular loss function is the `Mean Squared Function (MSE)\n",
    "`4. `updating parameters` to \n",
    "    - `improve accuracy` of our predictions (i.e. decrease difference between our predictions and data)\n",
    "    - `do it automatically`: the art of improving our model or `learning` is called `gradient descent`.  \n",
    "- `neural network`: Once the model is accuracy we are happy with, this is our `neural network`.\n",
    "\n",
    "Quite simple really?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulate Data\n",
    "Simulate content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch\n",
    "\n",
    "np.random.seed(42)\n",
    "def noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\n",
    "def add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n",
    "\n",
    "actual_x_values_tsr = torch.linspace(-2, 2, steps=20)[:,None] # simulate 20 actual x-values + shape to 2D tensor\n",
    "\n",
    "def actual_function(x): return 3*x**2 + 2*x + 1 \n",
    "# In reality we don't have a real function like this to use, \n",
    "# however we use this + add noise, which simulates real data\n",
    "# then we try to model this noisey data\n",
    "actual_y_values_unrealistic_tsr = actual_function(actual_x_values_tsr)\n",
    " # actual y-values of funtion we use to find\n",
    "\n",
    "#  but again, these values are not realistic because they're based on the real function - something that doenst exist in real life,\n",
    "# its like having the exact function that determines whether a photo is a cat or not\n",
    "# we can only approximate functions and its parameters\n",
    "# in real world, data has noise,\n",
    "# introduce data to these unrealistic real values to product realistic actual values\n",
    "\n",
    "# okay so why dont we need to add noise to actual_x_values? its any input is realistic/real world\n",
    "# any photo is can be asked 'is it a cat?'\n",
    "# any passenger with any characters can be asked 'did the passenger survive?\n",
    "\n",
    "actual_y_values_realistic_tsr = add_noise(actual_y_values_unrealistic_tsr, 0.15, 1.5) # use actualy y-values + add noise - to create simulated real data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we have a set of (actual realistic) 'data'\n",
    "# 1. try 'model' it with a quadratic equation \n",
    "# 2. create loss function - mean absolute error - difference between each point of actual y vs predicted y at each x, find difference and then absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_x_values[:5],actual_y_values_realistic[:5]\n",
    "\n",
    "# 1. Create quad function with parameters\n",
    "def quad_fn(a,b,c,x): return a*x**2 + b*x + c\n",
    "y_a1_b1_c1_x1 = quad_fn(1,1,1,1)\n",
    "y_a1_b1_c1_x1  # 3 = (1*1^2) + (1*1) + (1) = 1+1+1\n",
    "y_a1_b1_c1_x2 = quad_fn(1,1,1,2)\n",
    "y_a1_b1_c1_x2  # 7 = (1*2^2) + (2*1) + (2) = 4+2+1\n",
    "\n",
    "# Its quite cumbersome to put a new x-value through the function, to get a corresponding predicted y-value\n",
    "# ideally we can provide a list of xs to get a list of predicted ys (x-tensor -> f -> y-tensor)\n",
    "# and also the coefficients are parameterised\n",
    "from functools import partial\n",
    "def mk_quad_fn(a,b,c): return partial(quad_fn,a,b,c)\n",
    "quad111 = mk_quad_fn(1,1,1)\n",
    "# quad111(actual_x_values)\n",
    "predicted_quad111_y_values_tsr = quad111(actual_x_values_tsr) # remember is a 2d tensor due to added dimension with: [:,None]\n",
    "\n",
    "# we have actual-yand predicted-y data, lets compare them\n",
    "\n",
    "def mae(actual, preds): return abs(preds-actual).mean()\n",
    "\n",
    "mae(actual_y_values_realistic_tsr,predicted_quad111_y_values_tsr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.plot(actual_y_values_realistic_tsr,predicted_quad111_y_values_tsr)\n",
    "plt.scatter(actual_x_values_tsr, actual_y_values_realistic_tsr)\n",
    "\n",
    "# plot predictions\n",
    "# for predictions, graphically it will look better to plot a line \n",
    "# rather than just a coressponding y-prediction to each actual y\n",
    "\n",
    "# lets do just corresponding ones first to see what it looks like\n",
    "\n",
    "plt.scatter(actual_x_values_tsr, predicted_quad111_y_values_tsr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(actual_x_values_tsr, actual_y_values_realistic_tsr)\n",
    "# plt.scatter(actual_x_values_tsr, predicted_quad111_y_values_tsr)\n",
    "\n",
    "# plot y-line prediction\n",
    "\n",
    "plt.plot(actual_x_values_tsr, predicted_quad111_y_values_tsr, 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact(a=1,b=1,c=1)\n",
    "def plot_both(a,b,c):\n",
    "    # interactive_predicted_quad_y_values_tsr = custom_quad_fn(actual_x_values_tsr)\n",
    "\n",
    "    plt.scatter(actual_x_values_tsr, actual_y_values_realistic_tsr)\n",
    "\n",
    "    actual_x_values_for_plotting_tsr = torch.linspace(-2.1,2.1,100)[:,None]\n",
    "    custom_quad_fn = mk_quad_fn(a,b,c)\n",
    "    interactive_predicted_quad_y_values_tsr = custom_quad_fn(actual_x_values_for_plotting_tsr)\n",
    "    plt.ylim((-3,13))\n",
    "    plt.plot(actual_x_values_for_plotting_tsr, interactive_predicted_quad_y_values_tsr, 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f347031dbe44a7184565f5e30bc26bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.1, description='a', max=3.3000000000000003, min=-1.1), FloatSlider(vâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "from fastai.basics import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plt.rc('figure', dpi=90)\n",
    "\n",
    "def plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n",
    "    x = torch.linspace(min,max, 100)[:,None]\n",
    "    if ylim: plt.ylim(ylim)\n",
    "    plt.plot(x, f(x), color)\n",
    "    if title is not None: plt.title(title)\n",
    "\n",
    "def f(x): return 3*x**2 + 2*x + 1\n",
    "\n",
    "# plot_function(f, \"$3x^2 + 2x + 1$\")\n",
    "\n",
    "def quad(a, b, c, x): return a*x**2 + b*x + c\n",
    "\n",
    "def mk_quad(a,b,c): return partial(quad, a,b,c)\n",
    "\n",
    "\n",
    "# f2 = mk_quad(3,2,1)\n",
    "# plot_function(f2)\n",
    "\n",
    "def noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\n",
    "\n",
    "def add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x = torch.linspace(-2, 2, steps=20)[:,None]\n",
    "y = add_noise(f(x), 0.15, 1.5)\n",
    "@interact(a=1.1, b=1.1, c=1.1)\n",
    "def plot_quad(a, b, c):\n",
    "    plt.scatter(x,y)\n",
    "    plot_function(mk_quad(a,b,c), ylim=(-3,13))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectified_linear(m,b,x):\n",
    "    y = m*x+b\n",
    "    return torch.clip(y, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function(partial(rectified_linear, 1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def rectified_linear2(m,b,x): return F.relu(m*x+b)\n",
    "plot_function(partial(rectified_linear2, 1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(m=1.5, b=1.5)\n",
    "def plot_relu(m, b):\n",
    "    plot_function(partial(rectified_linear, m,b), ylim=(-1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_relu(m1,b1,m2,b2,x):\n",
    "    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n",
    "\n",
    "@interact(m1=-1.5, b1=-1.5, m2=1.5, b2=1.5)\n",
    "def plot_double_relu(m1, b1, m2, b2):\n",
    "    plot_function(partial(double_relu, m1,b1,m2,b2), ylim=(-1,6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
