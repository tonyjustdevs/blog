[
  {
    "objectID": "posts/2024-02-03-neural_network_basics/index.html",
    "href": "posts/2024-02-03-neural_network_basics/index.html",
    "title": "Neural Network Basics (Part 3)",
    "section": "",
    "text": "In Neural Network Basics: Part 2, the parameters of a function were found (optimised) to Minimise the Loss Function. The Loss Function chosen was the Mean Absolute Error, it could have been chosen to be the Mean Squared Error.\nBut What is the mathematical function if the wish to model something more complex like predicting the breed of Cat?\nUnfortunately, its unlikely the relationship between the parameters and whether a pixel is part of a Maine Coon üêà is a Quadratic, its going to be something more complicated.\nThankfully, there exists the infinitely flexible function known as Rectified Linear Unit (ReLU)"
  },
  {
    "objectID": "posts/2024-02-03-neural_network_basics/index.html#rectified-linear-unit-relu",
    "href": "posts/2024-02-03-neural_network_basics/index.html#rectified-linear-unit-relu",
    "title": "Neural Network Basics (Part 3)",
    "section": "1. Rectified Linear Unit (ReLU)",
    "text": "1. Rectified Linear Unit (ReLU)\n\nfrom ipywidgets import interact\nfrom fastai.basics import *\nfrom functools import partial\n\n\n1.1 Function\nThe function does two things:\n1. Calculate the output of a line\n2. If the output is smaller than zero, return zero\n\ndef rectified_linear(m,b,x):\n    y = m*x+b\n    return torch.clip(y,0.)\n\n\n\n1.2 Create A Custom ReLU Method\n\ndef custom_relu_fn(m,b): return partial(rectified_linear,m,b)\n\n\n\n1.3 Create y = 1x + 1 with Custom ReLU Method\n\nfn_11 = custom_relu_fn(1,1)\nfn_11\n\nfunctools.partial(&lt;function rectified_linear at 0x00000220331C9D00&gt;, 1, 1)\n\n\n\n\n1.4 ReLU y = 1x+ 1 Plot\n\nx = torch.linspace(-2.1,2.1,20)\nplt.plot(x,fn_11(x))\n\n\n\n\n\n\n\n\n\n1.4.1 Interactive ReLU\n\nplt.rc('figure', dpi=90)\n\n@interact(m=1.2, b=1.2)\ndef plot_relu(m, b):\n    min, max = -4.1, 4.1\n    x = torch.linspace(min,max, 100)[:,None]\n    fn_fixed = partial(rectified_linear, m,b)\n    ylim=(-1,4)\n    plt.ylim(ylim)\n    plt.axvline(0, color='gray', linestyle='dotted', linewidth=2)\n    plt.axhline(0, color='gray', linestyle='dotted', linewidth=2)\n    plt.plot(x, fn_fixed(x))\n\n\n\n\n\n\n\n\n1.5 Double ReLU Function\n\ndef double_relu(m1,b1,m2,b2,x):\n    return rectified_linear(m1,b1) + rectified_linear(m2,b2) \n\n\n1.5.1 Interactive Double ReLU\n\nplt.rc('figure', dpi=90)\n\ndef dbl_rectified_linear(m1, b1,m2,b2,x): \n    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n \n@interact(m1=-1.2, b1=-1.2,m2=1.2, b2=1.2)\ndef plot_dbl_relu(m1,b1,m2,b2):\n    min, max = -3.1, 3.1\n    x = torch.linspace(min,max, 100)[:,None]\n    fn_fixed = partial(dbl_rectified_linear, m1,b1,m2,b2)\n    ylim=(-1,4)\n    xlim=(-4,4)\n    plt.ylim(ylim)\n    plt.xlim(xlim)\n    plt.axvline(0, color='gray', linestyle='dotted', linewidth=2)\n    plt.axhline(0, color='gray', linestyle='dotted', linewidth=2)\n    plt.plot(x, fn_fixed(x))\n\n\n\n\n\n\n\n1.5.1 Triple ReLU for Good Measure!\n\ndef trple_rectified_linear(m1, b1, m2, b2, m3, b3, x): \n    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x) + rectified_linear(m3,b3,x)\n \n@interact(m1=-1.2, b1=-1.2,m2=1.2, b2=1.2, m3=0.5, b3=0.5)\ndef plot_trple_relu(m1,b1,m2,b2,m3,b3):\n# static variables\n    min, max = -3.1, 3.1\n    x = torch.linspace(min,max, 100)[:,None]\n    \n# update partial to include extra parameters m3, b3\n    triple_relu_fn_y = partial(trple_rectified_linear, m1,b1,m2,b2,m3,b3)\n\n# static variables\n    ylim=(-1,4) \n    xlim=(-4,4)\n    plt.ylim(ylim)\n    plt.xlim(xlim)\n    plt.axvline(0, color='gray', linestyle='dotted', linewidth=2)\n    plt.axhline(0, color='gray', linestyle='dotted', linewidth=2)\n    plt.plot(x, triple_relu_fn_y(x))"
  },
  {
    "objectID": "posts/2024-02-03-neural_network_basics/index.html#relu-is-an-infinitely-flexible-function",
    "href": "posts/2024-02-03-neural_network_basics/index.html#relu-is-an-infinitely-flexible-function",
    "title": "Neural Network Basics (Part 3)",
    "section": "2. ReLU is An Infinitely Flexible Function",
    "text": "2. ReLU is An Infinitely Flexible Function\nThere could be arbitrarily many ReLus added together to form any function!\nThe previous functions are of a single input x i.e.¬†2-Dimensions.\nReLU‚Äôs could be added together over as many dimensions as desired, i.e.¬†ReLU‚Äôs over surfaces or ReLU‚Äôs over 3D, 4D 5D etc.\nBut adding these ReLU‚Äôs, this means there are arbitrary amount of parameters related to each ReLU, how can these parameters be calculated?\nIn Part 2, a optimisation method called Gradient Descent was used to determine Parameters.\nThat‚Äôs Deep Learning in a nutshell. Beyond this, Tweaks are to:\n- make it faster\n- require less data"
  },
  {
    "objectID": "posts/2024-02-03-neural_network_basics/index.html#neural-network-basics-completed.",
    "href": "posts/2024-02-03-neural_network_basics/index.html#neural-network-basics-completed.",
    "title": "Neural Network Basics (Part 3)",
    "section": "Neural Network Basics Completed.",
    "text": "Neural Network Basics Completed.\nGo back to a previous post:\nNeural Network Basics: Part 1\nNeural Network Basics: Part 2\nNeural Network Basics: Part 3"
  },
  {
    "objectID": "posts/2024-01-31-99_neural_network_basics/index.html",
    "href": "posts/2024-01-31-99_neural_network_basics/index.html",
    "title": "Neural Network Basics (Part 1)",
    "section": "",
    "text": "A neural network is a mathematical function. So what‚Äôs that?\nA function is a mapping or transformation where each unique set of inputs is equal to exactly one output.\nIn highschool, the Vertical Line Test was used to determine whether a line was a function.\nThis post will go through basics of how to fit a line to some data."
  },
  {
    "objectID": "posts/2024-01-31-99_neural_network_basics/index.html#import-libraries",
    "href": "posts/2024-01-31-99_neural_network_basics/index.html#import-libraries",
    "title": "Neural Network Basics (Part 1)",
    "section": "1. Import Libraries",
    "text": "1. Import Libraries\n\nfrom ipywidgets import interact\nfrom fastai.basics import *\nimport pandas as pd"
  },
  {
    "objectID": "posts/2024-01-31-99_neural_network_basics/index.html#upload-and-plot-data",
    "href": "posts/2024-01-31-99_neural_network_basics/index.html#upload-and-plot-data",
    "title": "Neural Network Basics (Part 1)",
    "section": "2. Upload and Plot Data",
    "text": "2. Upload and Plot Data\n\ndf = pd.read_csv(\"upload_dataset.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n-2.000000\n11.869037\n\n\n1\n-1.789474\n6.543284\n\n\n2\n-1.578947\n5.939607\n\n\n3\n-1.368421\n2.630370\n\n\n4\n-1.157895\n1.794741\n\n\n\n\n\n\n\n\nplt.scatter(df.x, df.y)"
  },
  {
    "objectID": "posts/2024-01-31-99_neural_network_basics/index.html#quadratic-equation",
    "href": "posts/2024-01-31-99_neural_network_basics/index.html#quadratic-equation",
    "title": "Neural Network Basics (Part 1)",
    "section": "3. Quadratic Equation",
    "text": "3. Quadratic Equation\n\n3.1 General Quadratic Equation\n\ndef gen_quad_fn(a,b,c,x): return a*x**2 + b*x + c\n\n\n\n3.2 Custom Quadratric Equation\n\ndef custom_quad_fn(a,b,c): return partial(gen_quad_fn,a,b,c)\n\n\n\n3.3 Creating \\(1x^2 + 1x + 1\\)\n\nquad_111 = custom_quad_fn(1,1,1)\n\n\n\n3.4 Plotting \\(1x^2 + 1x + 1\\)\n\nxs_111 = df.x\nys_111 = quad_111(xs_111)\nplt.plot(xs_111,ys_111)\nplt.scatter(df.x, df.y)\n\n\n\n\n\n\n\n\n\n\n3.4 Interactive Quadratic Equation\nThe coefficients a, b and c of the Quadratic Function can be adjusted which in turn changes the shape of the line.\n[Future Iteration]: Figure out how to embed this adjustable plot into quarto blog\n\nplt.rc('figure', dpi=90)\n\n@interact(a=(0,5,0.1),b=(0,5,0.1),c=(0,5,0.1))\ndef interactive_plot(a,b,c):\n# 1. plot scatter\n    plt.scatter(df.x, df.y)    \n# 2. create xs_interact    \n    xs_interact = torch.linspace(-2.1,2.1,100)\n# 3. plot custom_quad_interactive\n    plt.ylim(-1,15)\n    plt.plot(xs_interact, custom_quad_fn(a,b,c)(xs_interact))\n\n\n\n\n\n\n\n3.5 Mean Absolute Errors (MAE)\nBy calculating a Loss Function such as Mean Absolute Errors, we can numerically determine what is the ‚Äòbest‚Äô fit of our line to the data.\nSure it isn‚Äôt entirely scientific to adjust it manually but its a good starting point.\n\ndef mae(prediction, actual): return np.mean(abs(prediction-actual))\n\n\nplt.rc('figure', dpi=90)\n\n@interact(a=(0,5,0.1),b=(0,5,0.1),c=(0,5,0.1))\ndef interactive_plot2(a,b,c):\n# 1.    plot scatter\n    plt.scatter(df.x, df.y)\n\n# 2     create custom_quad_interactive_fn\n# 2.1   create xs_interact    \n    xs_interact = torch.linspace(-2.1,2.1,100)\n\n# 3.    create ys_interact\n    plt.ylim(-1,15)\n    ys_interact = custom_quad_fn(a,b,c)(xs_interact)\n\n# 4.    calc mae\n    y_actual     = df.y\n    y_predicted  = custom_quad_fn(a,b,c)(df.x)\n    interact_mae = round(mae(y_actual, y_predicted),3)\n\n# 5. plot   \n    plt.plot(xs_interact, ys_interact)\n    plt.title(f\"MAE: {interact_mae}\")"
  },
  {
    "objectID": "posts/2024-01-31-99_neural_network_basics/index.html#to-be-continued",
    "href": "posts/2024-01-31-99_neural_network_basics/index.html#to-be-continued",
    "title": "Neural Network Basics (Part 1)",
    "section": "To be Continued‚Ä¶",
    "text": "To be Continued‚Ä¶\nThe next section go through a more automated method to find the smallest MAE.\nNeural Network Basics: Part 1\nNeural Network Basics: Part 2\nNeural Network Basics: Part 3"
  },
  {
    "objectID": "posts/2024-01-25-99_gradio_app/index.html",
    "href": "posts/2024-01-25-99_gradio_app/index.html",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 2)",
    "section": "",
    "text": "This post shows how to create a Gradio App (app.py) and run it locally in a browser. This app should:\nThis post is part of a series:\nPart 1: Create Learner (.pkl file)\nPart 2: Create Gradio application file (app.py)\nPart 3: Upload to HuggingFace account"
  },
  {
    "objectID": "posts/2024-01-25-99_gradio_app/index.html#part-2-create-gradio-application-file-app.py",
    "href": "posts/2024-01-25-99_gradio_app/index.html#part-2-create-gradio-application-file-app.py",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 2)",
    "section": "Part 2: Create Gradio application file (app.py)",
    "text": "Part 2: Create Gradio application file (app.py)"
  },
  {
    "objectID": "posts/2024-01-25-99_gradio_app/index.html#import-gradio-and-fast-ai-libraries",
    "href": "posts/2024-01-25-99_gradio_app/index.html#import-gradio-and-fast-ai-libraries",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 2)",
    "section": "1. Import Gradio and Fast AI libraries",
    "text": "1. Import Gradio and Fast AI libraries\n\nfrom fastai.vision.all import * \nimport gradio as gr\n\nc:\\Users\\tonyp\\miniconda3\\envs\\fastai\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn("
  },
  {
    "objectID": "posts/2024-01-25-99_gradio_app/index.html#import-learner-.pkl-file",
    "href": "posts/2024-01-25-99_gradio_app/index.html#import-learner-.pkl-file",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 2)",
    "section": "2. Import Learner (.pkl file)",
    "text": "2. Import Learner (.pkl file)\nRecall, this file was created and exported in Part 1\n\nIf you‚Äôre running on a Linux, you shouldn‚Äôt have any import issues.\n\nIf you‚Äôre running a Windows PC, you‚Äôll likely to experience an error.\n\n\npets_learner = load_learner('pets_learner.pkl') \n\nNotImplementedError: cannot instantiate 'PosixPath' on your system\n\n\n\n2.1 Import Learner Error (.pkl file) Solution (Windows only)\nI found a solution here.\nNot too sure why this happens, probably linux vs windows compatibility, forward vs backlashes probably?\nImport the pathlib library below and run the code below to fix the paths:\nNote:\n- This fix is only required during the testing phase of our Gradio App.\n- This testing phase is defined as being able to run the Gradio App locally.\n- When we Upload to HuggingFace Spaces, this code fix is not required (because HF is run on Linux, hence no Posix issues, from my understanding)\n\n# only run to import pkl in windows when youre doing testing in windows | when run in hus | its run via dock images ie linux ie no problems\nimport pathlib # \ntemp = pathlib.PosixPath\npathlib.PosixPath = pathlib.WindowsPath\n\n\npets_learner = load_learner('pets_learner.pkl')"
  },
  {
    "objectID": "posts/2024-01-25-99_gradio_app/index.html#predict-the-breed-with-imported-learner",
    "href": "posts/2024-01-25-99_gradio_app/index.html#predict-the-breed-with-imported-learner",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 2)",
    "section": "3 Predict the Breed with Imported Learner",
    "text": "3 Predict the Breed with Imported Learner\n\n3.1 Import Local Image\n\npet1 = PILImage.create('pet1.jpg')\npet1.thumbnail((224,224))\npet1\n\n\n\n\n\n\n\n\n\n\n3.2 Make prediction\nUse predict() to make prediction on the uploaded local image. The results will have 3 items:\n1. The prediction of the breed.\n2. Index of the Tensor (in point 3.)\n3. A Tensor of length 37. Why the odd number?\n- These are the probabilities of each unique breeds in our data!\n- Recall in Part 1, we determined the different categories by using a custom labelling function\n\nres = pets_learner.predict(pet1)\nres\n\nc:\\Users\\tonyp\\miniconda3\\envs\\fastai\\Lib\\site-packages\\fastai\\torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n  return getattr(torch, 'has_mps', False)\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n('Bombay',\n tensor(3),\n tensor([3.4475e-02, 1.7813e-03, 8.1388e-03, 6.3685e-01, 6.0993e-03, 4.8830e-04,\n         5.0747e-02, 8.8036e-03, 2.1084e-01, 3.4832e-02, 9.6598e-04, 7.6646e-04,\n         6.3764e-04, 2.5949e-04, 8.7999e-05, 1.3308e-03, 6.6650e-05, 5.7116e-05,\n         9.0377e-04, 6.9052e-05, 2.0095e-04, 2.1758e-05, 2.5767e-04, 2.1859e-05,\n         3.8547e-05, 1.6165e-06, 6.2601e-05, 4.3552e-05, 1.4901e-04, 9.6175e-05,\n         1.0010e-04, 1.3717e-04, 2.7684e-04, 5.5386e-05, 5.7012e-05, 4.6809e-05,\n         2.2643e-04]))\n\n\n\n\n3.3 What is the probabilities representing?\nThe probabilities tensor from predict() are the unique categories in our data and are stored in our dataloader\n\ncategories = pets_learner.dls.vocab \ncategories\n\n['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair', 'Egyptian_Mau', 'Maine_Coon', 'Persian', 'Ragdoll', 'Russian_Blue', 'Siamese', 'Sphynx', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond', 'leonberger', 'miniature_pinscher', 'newfoundland', 'pomeranian', 'pug', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier']\n\n\n\n\n3.4 The prediction and probabilities of our image\n\n# get index of predction\nidx = res[1]\n\n# store list of probalities of our predction\nprobabilities = res[2]\n\n# get breed and probability of our prediction\ncategories[idx],probabilities[idx]\n\n\n('Bombay', tensor(0.6369))\n\n\n\n\n3.5 Probability of every category available\n\ndef classify_image_fn(img):\n    prediction, idx, probabilities = pets_learner.predict(img)\n    return dict(zip(categories, map(float, probabilities)))\n\nclassify_image_fn(pet1)     # names and probabilities\n\nc:\\Users\\tonyp\\miniconda3\\envs\\fastai\\Lib\\site-packages\\fastai\\torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n  return getattr(torch, 'has_mps', False)\n\n\n\n\n\n\n\n\n\n{'Abyssinian': 0.034474823623895645,\n 'Bengal': 0.0017812795704230666,\n 'Birman': 0.008138809353113174,\n 'Bombay': 0.6368528604507446,\n 'British_Shorthair': 0.006099338177591562,\n 'Egyptian_Mau': 0.0004883029614575207,\n 'Maine_Coon': 0.05074741691350937,\n 'Persian': 0.008803554810583591,\n 'Ragdoll': 0.21084259450435638,\n 'Russian_Blue': 0.03483246639370918,\n 'Siamese': 0.0009659799397923052,\n 'Sphynx': 0.0007664564182050526,\n 'american_bulldog': 0.0006376394885592163,\n 'american_pit_bull_terrier': 0.0002594943216536194,\n 'basset_hound': 8.799932402325794e-05,\n 'beagle': 0.0013307805638760328,\n 'boxer': 6.664981629000977e-05,\n 'chihuahua': 5.711586709367111e-05,\n 'english_cocker_spaniel': 0.0009037724230438471,\n 'english_setter': 6.905203190399334e-05,\n 'german_shorthaired': 0.00020094779029022902,\n 'great_pyrenees': 2.1758336515631527e-05,\n 'havanese': 0.0002576670085545629,\n 'japanese_chin': 2.1859435946680605e-05,\n 'keeshond': 3.85471066692844e-05,\n 'leonberger': 1.616517351976654e-06,\n 'miniature_pinscher': 6.26009568804875e-05,\n 'newfoundland': 4.355219061835669e-05,\n 'pomeranian': 0.00014901049144100398,\n 'pug': 9.617456089472398e-05,\n 'saint_bernard': 0.0001000974079943262,\n 'samoyed': 0.0001371700782328844,\n 'scottish_terrier': 0.00027684049564413726,\n 'shiba_inu': 5.5386270105373114e-05,\n 'staffordshire_bull_terrier': 5.701235932065174e-05,\n 'wheaten_terrier': 4.680879646912217e-05,\n 'yorkshire_terrier': 0.0002264348149765283}"
  },
  {
    "objectID": "posts/2024-01-25-99_gradio_app/index.html#run-local-gradio-app",
    "href": "posts/2024-01-25-99_gradio_app/index.html#run-local-gradio-app",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 2)",
    "section": "4 Run local Gradio App",
    "text": "4 Run local Gradio App\nTheres a variety of ways to alter the app, I‚Äôve set the size of the images and provided some examples:\n\ngr_image = gr.Image(width=244, height=244)\ngr_label = gr.Label()\n\ninput_examples = ['pet1.jpg','pet2.jpg','pet3.jpg','pet4.jpg','pet5.jpg']\nintf = gr.Interface(fn=classify_image_fn,\n                    inputs=gr_image,    \n                    outputs=gr_label,\n                    examples=input_examples)\nintf.launch(inline=False)\n\nRunning on local URL:  http://127.0.0.1:7863\n\nTo create a public link, set `share=True` in `launch()`.\n\n\n\n\n\nc:\\Users\\tonyp\\miniconda3\\envs\\fastai\\Lib\\site-packages\\fastai\\torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n  return getattr(torch, 'has_mps', False)\nc:\\Users\\tonyp\\miniconda3\\envs\\fastai\\Lib\\site-packages\\fastai\\torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n  return getattr(torch, 'has_mps', False)\nc:\\Users\\tonyp\\miniconda3\\envs\\fastai\\Lib\\site-packages\\fastai\\torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n  return getattr(torch, 'has_mps', False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1 Open Local Gradio App\nOpen the URL provided in your favourite browser\n\n\n\n4.2 Test it out\nThe app knows a pug when it sees one! The app is ready."
  },
  {
    "objectID": "posts/2024-01-25-99_gradio_app/index.html#export-app.py",
    "href": "posts/2024-01-25-99_gradio_app/index.html#export-app.py",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 2)",
    "section": "5. Export (app.py)",
    "text": "5. Export (app.py)\nGradio app must be all inside a app.py script when being deployed on HuggingFaces.\nThankfully, there is a library to export all the cell-blocks from our notebook (.ipynb) to python script (app.py).\nReference\n\n5.1 Put directives at front of notebook\n\nPlace ‚Äò#| default_exp app‚Äô in a python codeblock in front of the the notebook\n\n\n\n\n5.2 Choose the cellblocks to export\n\nPlace ‚Äò#| export‚Äô in front of each codeblock you need to export\n\n\n\n\n5.3 Run nbdev and export\n\nfrom nbdev.export import nb_export\nnb_export('app.ipynb')\n\nThis will create a new folder as home directory and place in the app.py"
  },
  {
    "objectID": "posts/2024-01-25-99_gradio_app/index.html#to-be-continued",
    "href": "posts/2024-01-25-99_gradio_app/index.html#to-be-continued",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 2)",
    "section": "To be Continued‚Ä¶",
    "text": "To be Continued‚Ä¶\nPart 1: Create Learner (.pkl file)\nPart 2: Create Gradio application file (app.py)\nPart 3: Upload to HuggingFace account"
  },
  {
    "objectID": "posts/2024-01-24-99_multi_classifier/index.html",
    "href": "posts/2024-01-24-99_multi_classifier/index.html",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 1)",
    "section": "",
    "text": "I‚Äôve just created my first multi-category classifier using Jeremy Howard‚Äôs popular fast ai which is an astraction layer library built on top of the most world‚Äôs used deep-learning library PyTorch. I‚Äôve documented the process including the issues I faced (i.e.¬†bugs)\nI found it more easier to digest and understand this process by splitting the steps into 3 parts:\nPart 1: Create Learner (.pkl file)\nPart 2: Create Gradio application file (app.py)\nPart 3: Host on HuggingFace"
  },
  {
    "objectID": "posts/2024-01-24-99_multi_classifier/index.html#part-1-create-learner-.pkl-file",
    "href": "posts/2024-01-24-99_multi_classifier/index.html#part-1-create-learner-.pkl-file",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 1)",
    "section": "Part 1: Create Learner (.pkl file)",
    "text": "Part 1: Create Learner (.pkl file)"
  },
  {
    "objectID": "posts/2024-01-24-99_multi_classifier/index.html#install-and-import-libraries",
    "href": "posts/2024-01-24-99_multi_classifier/index.html#install-and-import-libraries",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 1)",
    "section": "1.1 Install and import libraries",
    "text": "1.1 Install and import libraries\n\n!pip install timm\n!pip install fastai \n\n\nfrom fastai.vision.all import *\nimport timm"
  },
  {
    "objectID": "posts/2024-01-24-99_multi_classifier/index.html#download-pets-breed-data",
    "href": "posts/2024-01-24-99_multi_classifier/index.html#download-pets-breed-data",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 1)",
    "section": "1.2 Download Pets Breed Data",
    "text": "1.2 Download Pets Breed Data\n\npath = untar_data(URLs.PETS)/'images'"
  },
  {
    "objectID": "posts/2024-01-24-99_multi_classifier/index.html#create-data-loader",
    "href": "posts/2024-01-24-99_multi_classifier/index.html#create-data-loader",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 1)",
    "section": "1.3 Create Data Loader",
    "text": "1.3 Create Data Loader\n\n1.3.1 (A different) Labelling Function\nHere a different method to label our data was used:\n\nIn ‚Äònoodles vs rice‚Äô model: There were two parent folders separating two categories of data: get_y=parent_label\nIn ‚Äòsaving a fast ai‚Äô model: There was a custom labelling function that looked for capital letters for cat breeds def is_cat(x): return x[0].isupper()\nIn this model, I used Regex to find breed names before the last ‚Äô_‚Äô in the file name: label_func=RegexLabeller(pat=r'^([^/]+)_\\d). See show_batch() output to see the file names examples.\n\nDid you notice this is the same dataset as the is_cat model? So changing our label resulted in a different model!\n\n\n1.3.2 Data Loader Code\n\npets_dataloaders =  ImageDataLoaders.from_name_func(\n    '.',\n    get_image_files(path),\n    valid_pct=0.2,\n    seed=42,\n    label_func=RegexLabeller(pat=r'^([^/]+)_\\d+'),\n    item_tfms=Resize(224))\n\nc:\\Users\\tonyp\\miniconda3\\envs\\fastai\\Lib\\site-packages\\fastai\\torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n  return getattr(torch, 'has_mps', False)"
  },
  {
    "objectID": "posts/2024-01-24-99_multi_classifier/index.html#batch-examples-create-learner-fine-tune-and-export",
    "href": "posts/2024-01-24-99_multi_classifier/index.html#batch-examples-create-learner-fine-tune-and-export",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 1)",
    "section": "1.4 Batch Examples, Create Learner, Fine-Tune and Export",
    "text": "1.4 Batch Examples, Create Learner, Fine-Tune and Export\nI grouped these steps as the code are exactly the same in previous posts.\n\n1.4.1 Batch Examples\nThis function is also a good way to find out what is the file name structure if we were not sure.\n\npets_dataloaders.show_batch(max_n=8)\n\n\n\n\n\n\n\n\n\n\n1.4.2 Create Learner\nI am still using resnet model architecture for starters for reasons mentioned previously by Jeremy Howard\n\npets_learner = vision_learner(pets_dataloaders, resnet34, metrics=error_rate)\n\n\n\n1.4.3 Fine-Tune\n\npets_learner.fine_tune(3) \n\n\n\n1.4.4 Export\n\npets_learner.export('pets_learner.pkl')"
  },
  {
    "objectID": "posts/2024-01-24-99_multi_classifier/index.html#to-be-continued",
    "href": "posts/2024-01-24-99_multi_classifier/index.html#to-be-continued",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 1)",
    "section": "To be Continued‚Ä¶",
    "text": "To be Continued‚Ä¶\nPart 1: Create Learner (.pkl file)\nPart 2: Create Gradio application file (app.py)\nPart 3: Host on HuggingFace"
  },
  {
    "objectID": "posts/2024-01-19-99_saving_a_fastai_model/index.html",
    "href": "posts/2024-01-19-99_saving_a_fastai_model/index.html",
    "title": "Saving a Fast AI Model",
    "section": "",
    "text": "This is a short tutorial to save (export) down a fast ai model (pkl file)."
  },
  {
    "objectID": "posts/2024-01-19-99_saving_a_fastai_model/index.html#load-fast-ai-libaries-and-download-dataset",
    "href": "posts/2024-01-19-99_saving_a_fastai_model/index.html#load-fast-ai-libaries-and-download-dataset",
    "title": "Saving a Fast AI Model",
    "section": "1. Load Fast AI Libaries and Download Dataset",
    "text": "1. Load Fast AI Libaries and Download Dataset\n\n!pip install -Uqq fastai\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.PETS)/'images'\n\n\npath\n\nPath('C:/Users/tonyp/.fastai/data/oxford-iiit-pet/images')\n\n\nIf you ran it in GoogleColab or Kaggle (recommended, its faster) then it‚Äôll be stored in the cloud. \nIf you ran it locally, its stored on your machine and you can take a look at the all the cute images! (Not recommended, its slow)"
  },
  {
    "objectID": "posts/2024-01-19-99_saving_a_fastai_model/index.html#labelling-function",
    "href": "posts/2024-01-19-99_saving_a_fastai_model/index.html#labelling-function",
    "title": "Saving a Fast AI Model",
    "section": "2. Labelling Function",
    "text": "2. Labelling Function\n\ndef is_cat(x): return x[0].isupper()\n\nOur data must be consistently labelled and parsed through into the model.\nFor this particular dataset, filenames starting with a Capital letter denotes a Cat, vice versa for a Non-Cat (Dog, in this case).\n\n\n\nPet Filenames\n\n\nLets write a function to handle the files names to get our labels (psuedo-code):\n1. Parse in file name and\n2. Obtain the first character and\n3. Check whether it is an upper case,\n4. If True, then it is a Cat.\nThere are various ways for us to supply the labelling to our model, in a previous blog Rice vs Noodles, the label was supplied via the parent folders name (rice folder and noodle folder).\n\nFast AI provides various helpful functions for common ways data is labelled to parse into our models\nFast AI Docs - Transforms - Label"
  },
  {
    "objectID": "posts/2024-01-19-99_saving_a_fastai_model/index.html#dataloader",
    "href": "posts/2024-01-19-99_saving_a_fastai_model/index.html#dataloader",
    "title": "Saving a Fast AI Model",
    "section": "3. DataLoader",
    "text": "3. DataLoader\nCreate the Dataloader and supply the labelling function we wrote into label_func.\n\ndls = ImageDataLoaders.from_name_func('.',\n    get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat,\n    item_tfms=Resize(192))\n\nc:\\Users\\tonyp\\miniconda3\\envs\\fastai\\Lib\\site-packages\\fastai\\torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n  return getattr(torch, 'has_mps', False)"
  },
  {
    "objectID": "posts/2024-01-19-99_saving_a_fastai_model/index.html#fine-tune-non-gpu-vs-gpu",
    "href": "posts/2024-01-19-99_saving_a_fastai_model/index.html#fine-tune-non-gpu-vs-gpu",
    "title": "Saving a Fast AI Model",
    "section": "4. Fine-tune (Non-GPU vs GPU)",
    "text": "4. Fine-tune (Non-GPU vs GPU)\nI attempted to fine-tune via Kaggle (GPU) and Locally (No GPU) and not suprisingly it is incredibly faster with a GPU setup.\nNvidia (and maybe other) GPUs are designed to be able to take multiple images at once (batches) grouped together (tensors) (I think 64 images at once), whereas a laptop without a GPU like mine will be processing 1 image at a time.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\nGPU took 35 seconds an epoch \nNon-GPU took 8 minutes an epoch"
  },
  {
    "objectID": "posts/2024-01-19-99_saving_a_fastai_model/index.html#export-the-model",
    "href": "posts/2024-01-19-99_saving_a_fastai_model/index.html#export-the-model",
    "title": "Saving a Fast AI Model",
    "section": "5. Export the model",
    "text": "5. Export the model\n\nlearn.export('catdogmodel.pkl')\n\nIn Kaggle, the model will be saved on their cloud and you can access it by using right-hand sidebar under Notebook -&gt; Data -&gt; Output\n\nIt‚Äôs only 46 Mb!, not too shabby!\n\nThats it! We‚Äôll go through how to use a saved/exported model in an upcoming post."
  },
  {
    "objectID": "posts/2024-01-18-99_rice_vs_noodles/index.html",
    "href": "posts/2024-01-18-99_rice_vs_noodles/index.html",
    "title": "Image Classifier 1: Noodles vs Rice",
    "section": "",
    "text": "Today I‚Äôll be attempting to build my first deep learning image classifier to distinguish between rice and noodles using knowledge gained from Jeremy Howards Fast AI course\nHigh-level steps:\n1. Search and Prepare Data\n2. Create DataLoader\n3. Create Learner\n4. Prediction\nI will detail any problems, issues, questions and resolutions during the process.\n\n!pip install -Uqq fastai\n\n\nfrom fastbook import * \n\nc:\\Users\\tonyp\\miniconda3\\envs\\fastai\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n\n\n\n1. Search and Prepare Data\n\n# 1.1 Get 'rice' photos\ndownload_url(search_images_ddg('rice',max_images=1)[0],'rice.jpg',show_progress=False)\nImage.open('rice.jpg').to_thumb(256,256)\n\n\n\n\n\n\n\n\n\n# 1.2 Get 'noodles' photos\ndownload_url(search_images_ddg('noodles', max_images=1)[0],'noodles.jpg',show_progress=False)\nImage.open('noodles.jpg').to_thumb(256,256)\n\n\n\n\n\n\n\n\nLets use 60 imagess of ‚Äòrice‚Äô and ‚Äònoodles‚Äô from DuckDuckGo.\nNote: I downloaded for 100 images of each and then taking 60 of them as some images fail so I‚Äôm leaving room for failed photos.\nQuestion: Why do we need verify and why do some photos fail?\n\n# 1.3 Prep images in folders\nsearches = ['rice', 'noodles']\npath = Path('rice_or_noodles')\n\nif not path.exists(): # Ensure the path exists\n    for o in searches:\n        dest = (path/o)\n        dest.mkdir(parents=True, exist_ok=True)\n        print(f'Searching for {o} images...')\n        results = search_images_ddg(f'{o} photo',max_images=100)\n        print(f'{len(results)} images found for {o}. Downloading...')\n        download_images(dest, urls=results[:60])\n        print(f'Resizing images in {dest}')\n        resize_images(dest, max_size=400, dest=dest)\n\n\n# 1.4 Remove Failed images\npath = Path('rice_or_noodles')\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\n\n(#0) []\n\n\n\n\n2. Create DataLoader\n\n# 2.1 \ndls = DataBlock(\n    blocks = (ImageBlock, CategoryBlock), # i.e.input image / ouput is category (coin or notes)\n    get_items = get_image_files, # returns list of images files\n    splitter = RandomSplitter(valid_pct=0.2, seed=42), # critical to test accuracy with validation set\n    get_y=parent_label, # use parents folder of a path\n    item_tfms=[Resize(192, method=\"squish\")] # most computer vision architecutres need all your inputs to be same size \n).dataloaders(path) \n\nc:\\Users\\tonyp\\miniconda3\\envs\\fastai\\Lib\\site-packages\\fastai\\torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n  return getattr(torch, 'has_mps', False)\n\n\n\n# 2.2 We can see Paths were created for every image and split into our training and data sets\ndls.train_ds.items[:2]\ndls.valid_ds.items[:2]\n\n[Path('rice_or_noodles/rice/4280fe58-691a-4c0b-85a5-5c1c8400ecb7.jpg'),\n Path('rice_or_noodles/rice/f8a77d77-c007-4854-af8b-2af624a8da66.jpg')]\n\n\n[Question]: How does it know whether it is training set or valid set? I guess theres some indexing somewhere that I dont know how to obtain.\n\n# 2.1 Show a training batch which has an 'image' and a 'label'\ndls.show_batch(max_n=6) #batch shows input and label\n\n\n\n\n\n\n\n\n\n\n2. Create Learner using ResNet\nIn the course, we used a pre-trained model ‚ÄòResNet18‚Äô (RN).\nWhy Pre-trained Models?:\n- Pre-trained models is like getting an athlete who is very good basic sport related skills like hand-eye coordination, jumping, running/sprinting, changing directions etc and then - telling them to learn a specific sport (fine-tuning), - say tennis (labelled dataset provided). With a good base of skills, this person should be able to learn tennis to a good level‚Ä¶\nResNet18:\n- ResNet18 is trained on 1.28 million images with 1000 object categories. - 18 layers\n- Trained on ImageNet dataset\n[Future iterations 1]: Perhaps there are alternative pre-trained models specialising in food?\n[Future iterations 2]: - Read up and try understand the various architectures Fast AI‚Äôs TIMM model architectures - Try different architectures and different versions\n\nlearner_RN18 = vision_learner(dls, resnet18, metrics=error_rate)\n\n\n2.1 Learner Model Times:\nThey all took under 10 seconds to create the general learner. Now to fine-tune them!\n\nlearner_RN18.fine_tune(8)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.840357\n4.676042\n0.476190\n00:03\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.763106\n3.761843\n0.476190\n00:04\n\n\n1\n1.517361\n2.798523\n0.476190\n00:04\n\n\n2\n1.202234\n2.308116\n0.428571\n00:04\n\n\n3\n0.953227\n1.637496\n0.428571\n00:04\n\n\n4\n0.770979\n1.034023\n0.380952\n00:04\n\n\n5\n0.662257\n0.641428\n0.190476\n00:04\n\n\n6\n0.563239\n0.405057\n0.142857\n00:04\n\n\n7\n0.490904\n0.285846\n0.095238\n00:04\n\n\n\n\n\n\nOur learner is performing at 90% accuracy (9% error rate) by looking at only 60 photos!\nLets try predict some random photos of rice and noodles I‚Äôve found on the internet.\n\nfrom IPython.display import Image # import image viewer\n\n\n# noodle predictor\nuploader = SimpleNamespace(data = ['test_noodle.jpg'])\nimage_path = uploader.data[0]\ndisplay(Image(filename=image_path))\nres1, res2, res3 = learner_RN18.predict(image_path)\nprint(f\"{res1}: {res3[res2]*100:.2f}%\")\n\n\n\n\n\n\n\n\nc:\\Users\\tonyp\\miniconda3\\envs\\fastai\\Lib\\site-packages\\fastai\\torch_core.py:263: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n  return getattr(torch, 'has_mps', False)\n\n\n\n\n\n\n\n\n\nnoodles: 99.98%\n\n\n\n\nPrediction 1: Noodles\nThe model predicted noodles correctly with 99.98% confidence!\n\n# rice predictor 1\nuploader = SimpleNamespace(data = ['test_rice.jpg'])\nimage_path = uploader.data[0]\ndisplay(Image(filename=image_path))\n\nres1, res2, res3 = learner_RN18.predict(image_path)\nprint(f\"{res1}: {res3[res2]*100:.2f}%\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnoodles: 66.22%\n\n\n\n\nPrediction and Results 2: Rice 1\nThe model predicted rice incorrectly with 66.22% confidence!\nI was a bit confused so I decided to provide another image of rice to make\n\n# rice predictor 2\nuploader = SimpleNamespace(data = ['test_rice2.jpg'])\nimage_path = uploader.data[0]\ndisplay(Image(filename=image_path)) # show image\n\n# get\nres1, res2, res3 = learner_RN18.predict(image_path)\nprint(f\"{res1}: {res3[res2]*100:.2f}%\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnoodles: 98.51%\n\n\n\n\nPrediction and Results 3: Rice 2\nThe model predicted rice incorrectly with 98.51% confidence!\nOkay now there is clearly something wrong going on. I decide to take a gander at the photos in my ‚Äòrice‚Äô folder.\n\nIt looks like we‚Äôve trained a learner specialises in bowled or white rice. I was testing the model with fried rice since that is my favourite rice dish.\nLets test out a couple photos on bowled rice.\n\n# rice predictor 2\nuploader1 = SimpleNamespace(data = ['test_boiledrice1.jpg'])\nuploader2 = SimpleNamespace(data = ['test_boiledrice2.jpg'])\nimage_path1 = uploader1.data[0]\nimage_path2 = uploader2.data[0]\n\ndisplay(Image(filename=image_path1)) # show image\ndisplay(Image(filename=image_path2)) # show image\n\nres1, res2, res3 = learner_RN18.predict(image_path1)\nprint(f\"{res1}: {res3[res2]*100:.2f}%\")\nres1, res2, res3 = learner_RN18.predict(image_path2)\nprint(f\"{res1}: {res3[res2]*100:.2f}%\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrice: 88.73%\nnoodles: 92.57%\n\n\n\n\n\n\n\n\n\nNow I‚Äôm confused as its predicting incorrectly with 92.57% confidence.\nPerhaps the model isnt seeing enough data?\nLets train a new model with:\n- 300 images instead of 60\n- ‚Äòrice food‚Äô and ‚Äònoodle food‚Äô as keyword insteads of just ‚Äòrice‚Äô and ‚Äònoodles‚Äô\n\nsearches = ['rice food', 'noodles food']\npath_200 = Path('rice_or_noodles_300')\n\nif not path_200.exists(): # Ensure the path exists\n    for o in searches:\n        dest = (path_200/o)\n        dest.mkdir(parents=True, exist_ok=True)\n        print(f'Searching for {o} images...')\n        results = search_images_ddg(f'{o} photo',max_images=300)\n        print(f'{len(results)} images found for {o}. Downloading...')\n        download_images(dest, urls=results[:200])\n        print(f'Resizing images in {dest}')\n        resize_images(dest, max_size=400, dest=dest)\n\n\n\n# 1.4 Remove Failed images\npath_200 = Path('rice_or_noodles_300')\nfailed = verify_images(get_image_files(path_200))\nfailed.map(Path.unlink)\n\n\n(#10) [None,None,None,None,None,None,None,None,None,None]\n\n\n\n\ndls_200 = DataBlock(\n    blocks = (ImageBlock, CategoryBlock), # i.e.input image / ouput is category (coin or notes)\n    get_items = get_image_files, # returns list of images files\n    splitter = RandomSplitter(valid_pct=0.2, seed=42), # critical to test accuracy with validation set\n    get_y=parent_label, # use parents folder of a path\n    item_tfms=[Resize(192, method=\"squish\")] # most computer vision architecutres need all your inputs to be same size \n).dataloaders(path_200) \n\n\nlearner_RN18_200 = vision_learner(dls_200, resnet18, metrics=error_rate)\n\n\nlearner_RN18_200.fine_tune(4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.155098\n0.872050\n0.338462\n00:11\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.625260\n0.402908\n0.169231\n00:15\n\n\n1\n0.442973\n0.289800\n0.138462\n00:14\n\n\n2\n0.317375\n0.328805\n0.153846\n00:14\n\n\n3\n0.235606\n0.327507\n0.123077\n00:15\n\n\n\n\n\n\n# Prediction with new learner (300 images and specific keywords)\n# rice predictor 2\nuploader1 = SimpleNamespace(data = ['test_boiledrice1.jpg'])\nuploader2 = SimpleNamespace(data = ['test_boiledrice2.jpg'])\nimage_path1 = uploader1.data[0]\nimage_path2 = uploader2.data[0]\n\ndisplay(Image(filename=image_path1)) # show image\ndisplay(Image(filename=image_path2)) # show image\n\nres1, res2, res3 = learner_RN18_200.predict(image_path1)\nprint(f\"{res1}: {res3[res2]*100:.2f}%\")\nres1, res2, res3 = learner_RN18_200.predict(image_path2)\nprint(f\"{res1}: {res3[res2]*100:.2f}%\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrice food: 100.00%\nrice food: 99.95%\n\n\n\n\n\n\n\n\n\nSo it‚Äôs now 100 and 99.95% confident they‚Äôre rice, which is great!\nLets try some fried rice!\nWe‚Äôll retest now at the fried rice photo which the initial model guessed to be noodles with 98.5% confidence\n\n# Prediction with new learner (300 images and specific keywords)\n# rice predictor 2\nuploader1 = SimpleNamespace(data = ['test_rice2.jpg'])\nimage_path1 = uploader1.data[0]\n\ndisplay(Image(filename=image_path1)) \n\nres1, res2, res3 = learner_RN18_200.predict(image_path1)\nprint(f\"{res1}: {res3[res2]*100:.2f}%\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrice food: 99.56%\n\n\nGreat! It is correct with 99.56% confidence.\nI think we‚Äôve created a great rice and noodles classifier, lets stop here.\n[Future Iteration 3]: Build web app for everyone to test it out\n[Future Iteration 4]: Make it useable on my blog\n[Question] I wonder if theres a way to quickly see all specific headings I‚Äôve used, I find myself scrolling up and download to find what Iteration I‚Äôm up to‚Ä¶\nApologies for the lack of neatness, lets hope this improves over time‚Ä¶"
  },
  {
    "objectID": "posts/2024-01-16-98_post_without_code/index.html",
    "href": "posts/2024-01-16-98_post_without_code/index.html",
    "title": "Post Without Code",
    "section": "",
    "text": "Learning how to quarto blog.\nThis is a post with just this sentence."
  },
  {
    "objectID": "posts/2024-01-16-96_post_with_git_clone/index.html",
    "href": "posts/2024-01-16-96_post_with_git_clone/index.html",
    "title": "Post With Git Clone",
    "section": "",
    "text": "Learning how git work and\nthis is a post initiated by cloning existing repo."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Oreo",
    "section": "",
    "text": "About Tony\nThis is a blog to note down my learning journey. Perhaps it can help others too.\nPoem of Oreo\nIn a bustling town of Ultimo, where city lights gleam,\nA tale unfolds of Oreo, a kitten‚Äôs sweet dream.\nTony brought him home with Lilo by his side,\nA tuxedo cat, in black and white pride.\nOreo, a Maine Coon with a fluffy coat,\nBlack all over, with white around his throat.\nWhite paws that dance, a playful delight,\nA mischievous gleam in his eyes, shining bright.\nLilo, a tiny British Shorthair so fair,\nA dainty companion with a gentle air.\nThey grew up together, a dynamic pair,\nFrom Ultimo to Pyrmont, a journey to share.\nThrough the streets of Townhall, they explored,\nAdventures aplenty, their spirits soared.\nYet, runaway moments were a frequent feat,\nFound and embraced, their connection so sweet.\nOreo, a rogue, with a penchant for bins,\nA greedy delight, where the treasure begins.\nFeasting on scraps, his appetite vast,\nBut his cuteness prevails, a spell he has cast.\nLilo, petite, with a modest cuisine,\nA nibble here, a delicate routine.\nShe watches Oreo with curious eyes,\nAs he plays around, chasing butterflies.\nIn the city‚Äôs heartbeat, their story unfolds,\nThrough alleys and parks, where the tale molds.\nOreo, the player, with antics so grand,\nLilo, the watcher, in the city so grand.\nNow, in the world of influencers and fame,\nOreo has found his claim to the game.\nAn influencer cat, with followers galore,\nFrom bins to glamour, a journey to adore.\nThrough Ultimo, Pyrmont, and Townhall‚Äôs embrace,\nOreo and Lilo found their special place.\nA tale of friendship, of mischief and grace,\nIn the city‚Äôs heartbeat, a memory to trace."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Neural Network Basics (Part 3)\n\n\n\n\n\n\nbasics\n\n\nrelu\n\n\n\nCreating the ReLU Function \n\n\n\n\n\nFeb 3, 2024\n\n\nTony Phung\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network Basics (Part 2)\n\n\n\n\n\n\nbasics\n\n\ngradient descent\n\n\n\nOptimising with Gradient Descent\n\n\n\n\n\nFeb 2, 2024\n\n\nTony Phung\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network Basics (Part 1)\n\n\n\n\n\n\nbasics\n\n\n\nManually fitting a Line (Quadratic Function) to a dataset\n\n\n\n\n\nJan 31, 2024\n\n\nTony Phung\n\n\n\n\n\n\n\n\n\n\n\n\nHow To Setup a Kaggle API\n\n\n\n\n\n\nkaggle\n\n\n\nContinuing my Data Science journey with Kaggle\n\n\n\n\n\nJan 27, 2024\n\n\nTony Phung\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 3)\n\n\n\n\n\n\nhuggingface\n\n\n\nHost a neural network app live on HuggingFace\n\n\n\n\n\nJan 25, 2024\n\n\nTony Phung\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 2)\n\n\n\n\n\n\ngradio\n\n\n\nCreate a local neural network Gradio App\n\n\n\n\n\nJan 25, 2024\n\n\nTony Phung\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 1)\n\n\n\n\n\n\nimage classifier\n\n\n\nCreate a simple neural network model\n\n\n\n\n\nJan 24, 2024\n\n\nTony Phung\n\n\n\n\n\n\n\n\n\n\n\n\nHow to choose a different Deep-Learning Model Architecture\n\n\n\n\n\n\ntimm\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nTony Phung\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying My First Live App & it‚Äôs a Neural Network!\n\n\n\n\n\n\ngradio\n\n\nhuggingface\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nTony Phung\n\n\n\n\n\n\n\n\n\n\n\n\nSaving a Fast AI Model\n\n\n\n\n\n\nfast ai\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nTony Phung\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classifier 1: Noodles vs Rice\n\n\n\n\n\n\nimage classifier\n\n\nfast ai\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\nTony Phung\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Git Clone\n\n\n\n\n\n\ntesting quarto\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nTony Phung\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Sample Jupyter Notebook\n\n\n\n\n\n\ntesting quarto\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nTony Phung\n\n\n\n\n\n\n\n\n\n\n\n\nPost Without Code\n\n\n\n\n\n\ntesting quarto\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nTony Phung\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\ntesting quarto\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nTony Phung\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-01-16-97_post_with_notebook/index.html",
    "href": "posts/2024-01-16-97_post_with_notebook/index.html",
    "title": "Post With Sample Jupyter Notebook",
    "section": "",
    "text": "Learning how quarto works with jupyter notebooks. This is a sample editted notebook from fastai.\n\nfrom fastai.vision.all import *\nchosen_sample_seed          = 42\nchosen_sample_n             = 5\n\nc:\\Users\\tonyp\\miniconda3\\envs\\fastai\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n\n\n\n# useful informative functions\ndef print_useful_info():\n    global path_dir_obj, list_all_img_path_obj\n    print()\n    print(f\"path_obj_home_dir: \\t{path_dir_obj}\")\n    print(f\"image_list_count: \\t{len(list_all_img_path_obj)}\")\n    return None\n\ndef print_sample_imgs():\n    global path_dir_obj, list_all_img_path_obj, chosen_sample_seed, chosen_sample_n\n    import random\n    set_seed(chosen_sample_seed)\n    print(f\"set_seed_number: \\t{chosen_sample_seed}\")\n    rng         = len(list_all_img_path_obj)-1  # Replace 10 with the desired upper limit (exclusive)\n    random_nos  = random.sample(range(rng), chosen_sample_n)\n    print()\n    print(\"sample_images:\")\n    for index, img_path in enumerate(list_all_img_path_obj[random_nos]):\n        print(f\" {random_nos[index]:&gt;7}: \\t\\t{img_path}\")\n    \n    \n    for image_path in list_all_img_path_obj[random_nos]:\n        img = PILImage.create(image_path)\n        show_image(img)\n    return None\n\n\n# 0. get paths of images\npath_dir_obj                = untar_data(URLs.MNIST_TINY)\nlist_all_img_path_obj       = get_image_files(path_dir_obj)\n\n\n# 1. create learner\ndata_loader = ImageDataLoaders.from_folder(path_dir_obj, \n                                    img_cls=PILImageBW,\n                                    set_seed=42)\nx1,y1 = data_loader.one_batch()\ntest_eq(x1.shape, [64, 1, 28, 28])\n\nprint_useful_info()\nprint_sample_imgs()\n# check valid data sets - can check if splits are as expected\nprint(len(data_loader.valid_ds.items)) # 699 as expected\nprint(len(data_loader.train_ds.items)) # 709 as expected\n\n# can show sample pics\ndata_loader.show_batch() #show examples?\n\n\npath_obj_home_dir:  C:\\Users\\tonyp\\.fastai\\data\\mnist_tiny\nimage_list_count:   1428\nset_seed_number:    42\n\nsample_images:\n    1309:       C:\\Users\\tonyp\\.fastai\\data\\mnist_tiny\\valid\\7\\9036.png\n     228:       C:\\Users\\tonyp\\.fastai\\data\\mnist_tiny\\train\\3\\8830.png\n      51:       C:\\Users\\tonyp\\.fastai\\data\\mnist_tiny\\train\\3\\731.png\n     563:       C:\\Users\\tonyp\\.fastai\\data\\mnist_tiny\\train\\7\\868.png\n     501:       C:\\Users\\tonyp\\.fastai\\data\\mnist_tiny\\train\\7\\8186.png\n699\n709\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnbr_learner = vision_learner(data_loader, resnet34, metrics=error_rate)\n\n\nnbr_learner.fine_tune(4)\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/11 00:00&lt;?]\n    \n    \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.303663\n0.174878\n0.054363\n00:21\n\n\n1\n0.228222\n0.127563\n0.040057\n00:20\n\n\n2\n0.172806\n0.091346\n0.027182\n00:21\n\n\n3\n0.139439\n0.056558\n0.015737\n00:21\n\n\n\n\n\n\nfrom IPython.display import Image # import image viewer\n\n\n\nuploader = SimpleNamespace(data = ['3.png'])\nimage_path = uploader.data[0]\nImage(filename=image_path)\nres1, res2, res3 = nbr_learner.predict(image_path) # predict unseen input using LEARNER\nprint(res1,res2,res3, sep=\"\\n\") #fix output formatting later\nprint(\"correctly guessed the [0], representing [3]\")\n\n\n\n\n\n\n\n\n3\ntensor(0)\ntensor([9.9998e-01, 2.1658e-05])\ncorrectly guessed the [0], representing [3]\n\n\n\n\nuploader = SimpleNamespace(data = ['7.png'])\nimage_path = uploader.data[0]\nImage(filename=image_path)\nres1, res2, res3 = nbr_learner.predict(image_path) # predict unseen input using LEARNER\nprint(res1,res2,res3, sep=\"\\n\") #fix output formatting later\nprint(\"correctly guessed the [1], representing [7]\")\n\n\n\n\n\n\n\n\n\n7\ntensor(1)\ntensor([0.0050, 0.9950])\ncorrectly guessed the [1], representing [7]"
  },
  {
    "objectID": "posts/2024-01-16-99_welcome/index.html",
    "href": "posts/2024-01-16-99_welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nDefault Quarto content:\n‚ÄúSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts.‚Äù‚Äù"
  },
  {
    "objectID": "posts/2024-01-19-98_embed_gradio_hgface/index.html#introduction",
    "href": "posts/2024-01-19-98_embed_gradio_hgface/index.html#introduction",
    "title": "Deploying My First Live App & it‚Äôs a Neural Network!",
    "section": "1. Introduction",
    "text": "1. Introduction\nEmbedding a classic Convolutional Neural Network (CNN) Gradio ‚ÄòCat versus Dog‚Äô classifier Gradio App, hosted on HuggingFace Spaces, into my Quarto Blog. What a mouthful!"
  },
  {
    "objectID": "posts/2024-01-19-98_embed_gradio_hgface/index.html#background",
    "href": "posts/2024-01-19-98_embed_gradio_hgface/index.html#background",
    "title": "Deploying My First Live App & it‚Äôs a Neural Network!",
    "section": "2. Background",
    "text": "2. Background\nThis App will guess whether an image is a Cat or Dog with a level of confidence using a deep learning neural network based on 700 mbs of labelled photos of dogs and cats. I‚Äôll post more information on the model itself in a different post.\nAll doggosüêï & catsüêà image examples has never been viewed by the Model and it makes a prediction in milliseconds! Images supplied by my good friends in Australia ü¶ò and Vietnam üçú. Thanks guys!\nThe App isn‚Äôt perfect:\n- It guesses Incorrectly with great 95% confidence a cat as a dog!\nI‚Äôm thrilled to be able to build the app, get it hosted and embed it all in one day for the first time! Alot of firsts today!"
  },
  {
    "objectID": "posts/2024-01-19-98_embed_gradio_hgface/index.html#future-stuff",
    "href": "posts/2024-01-19-98_embed_gradio_hgface/index.html#future-stuff",
    "title": "Deploying My First Live App & it‚Äôs a Neural Network!",
    "section": "3. Future stuff",
    "text": "3. Future stuff\n[1]: Multi-Image Uploader + Predictor.\n[2]: Explain how I built the Gradio App, got it hosted Hugging Face, and embedded here.\n[4]: Upgrade the rice vs noodle model an App and hosted.\n[4]: Learn some HTML/CSS to make things prettier."
  },
  {
    "objectID": "posts/2024-01-19-98_embed_gradio_hgface/index.html#heres-the-app",
    "href": "posts/2024-01-19-98_embed_gradio_hgface/index.html#heres-the-app",
    "title": "Deploying My First Live App & it‚Äôs a Neural Network!",
    "section": "4. Heres the App!",
    "text": "4. Heres the App!"
  },
  {
    "objectID": "posts/2024-01-23-99-testing-different_archs/index.html",
    "href": "posts/2024-01-23-99-testing-different_archs/index.html",
    "title": "How to choose a different Deep-Learning Model Architecture",
    "section": "",
    "text": "Today I‚Äôll go through how to find and test different deep-learning architectures from Pytorch Image Models (timm) library made available here by Ross Wightman and use them in our models."
  },
  {
    "objectID": "posts/2024-01-23-99-testing-different_archs/index.html#using-timm---pytorch-image-models",
    "href": "posts/2024-01-23-99-testing-different_archs/index.html#using-timm---pytorch-image-models",
    "title": "How to choose a different Deep-Learning Model Architecture",
    "section": "1. Using timm - PyTorch Image Models",
    "text": "1. Using timm - PyTorch Image Models\n\n1.1 Introduction\nWhat are timm‚Äôs models? They‚Äôre mathematical functions (i.e.¬†application of matrix multiplication, non-linearities e.g.¬†ReLu‚Äôs)\nReference: ‚ÄúWhich image model are best‚Äù - Jeremy Howard\nReference: ‚Äútimm‚Äù - Ross Wightman\nWe want to know 3 things:\n1. how fast are they? You‚Äôd want models in top left of chart.\n2. how much memory?\n3. how accurate are they? Lower error rate the better.\n[Future iteration I]: A formalised metholodgy to decide what is fast enough, appropriate memory-use, and what is accurate enough for our use-cases.\nThere is a useful high-level chart from Jeremy‚Äôs notebook charting accuracy (Y-axis) vs secs per sample (X-axis):\n\nI chose to use a model from the convnext family due to its balance of high accuracy and speed.\n[Future iteration II]: Some more formalised methodology on choosing the architecture. Jeremy does mention architecture should be the one last thing things to worry about and he usually builds from resnet and tests whether it is, accurate enough and fast enough, then iterate from there.\n\n\n1.2 Import timm library\n\nimport timm\n\n\n\n1.3 List available model architectures and choose one\n\ntimm.list_models('convnext*') # * wild card searches \n\n['convnext_atto',\n 'convnext_atto_ols',\n 'convnext_base',\n 'convnext_femto',\n 'convnext_femto_ols',\n 'convnext_large',\n 'convnext_large_mlp',\n 'convnext_nano',\n 'convnext_nano_ols',\n 'convnext_pico',\n 'convnext_pico_ols',\n 'convnext_small',\n 'convnext_tiny',\n 'convnext_tiny_hnf',\n 'convnext_xlarge',\n 'convnext_xxlarge',\n 'convnextv2_atto',\n 'convnextv2_base',\n 'convnextv2_femto',\n 'convnextv2_huge',\n 'convnextv2_large',\n 'convnextv2_nano',\n 'convnextv2_pico',\n 'convnextv2_small',\n 'convnextv2_tiny']"
  },
  {
    "objectID": "posts/2024-01-23-99-testing-different_archs/index.html#create-your-learner-with-a-timm-model",
    "href": "posts/2024-01-23-99-testing-different_archs/index.html#create-your-learner-with-a-timm-model",
    "title": "How to choose a different Deep-Learning Model Architecture",
    "section": "2. Create your Learner with a timm model",
    "text": "2. Create your Learner with a timm model\n\n2.1 Get your data\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)/'images'\n\n\n\n2.2 Prepare your Functions\n\ndef is_cat(x): return x[0].isupper()\n\n\n\n2.3 Load your DataLoader\n\ndls = ImageDataLoaders.from_name_func('.',\n    get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat,\n    item_tfms=Resize(192))\n\n\n\n2.4 Build your Learner\n\nlearn_conv = vision_learner(dls, 'convnext_tiny', metrics=error_rate).to_fp16()\nlearn_resn = vision_learner(dls, 'resnet18', metrics=error_rate).to_fp16()\n\n\n\n2.5 Fine-Tune: ResNet18 vs ConvNextTiny\nA 90% reduction in the error rate! (0.6766% to 0.0667%: 1-(0.000677/0.006766)). It‚Äôs noted that the resnet error rate was quite low and changing the model was probably not necessary."
  },
  {
    "objectID": "posts/2024-01-25-98_huggingface/index.html",
    "href": "posts/2024-01-25-98_huggingface/index.html",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 3)",
    "section": "",
    "text": "This post shows how to host your working Local Gradio App on HuggingFace.\nThis post is part of a series:\nPart 1: Create Learner (.pkl file)\nPart 2: Create Gradio application file (app.py)\nPart 3: Host on HuggingFace account"
  },
  {
    "objectID": "posts/2024-01-25-98_huggingface/index.html#part-3-host-on-huggingface-account",
    "href": "posts/2024-01-25-98_huggingface/index.html#part-3-host-on-huggingface-account",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 3)",
    "section": "Part 3: Host on HuggingFace account",
    "text": "Part 3: Host on HuggingFace account"
  },
  {
    "objectID": "posts/2024-01-25-98_huggingface/index.html#create-huggingface-account-and-create-a-space",
    "href": "posts/2024-01-25-98_huggingface/index.html#create-huggingface-account-and-create-a-space",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 3)",
    "section": "1 Create HuggingFace Account and Create a ‚ÄòSpace‚Äô",
    "text": "1 Create HuggingFace Account and Create a ‚ÄòSpace‚Äô\n\nChoose your Space name\nChoose Apache-2.0 to avoid any copyright issues\nChoose Gradio\nChoose the Free option\nChoose Public (show you can show it to the world!)"
  },
  {
    "objectID": "posts/2024-01-25-98_huggingface/index.html#clone-the-repo",
    "href": "posts/2024-01-25-98_huggingface/index.html#clone-the-repo",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 3)",
    "section": "2 Clone the repo",
    "text": "2 Clone the repo\nThis will create allow us deploy the Gradio app to the HuggingFace repository:\ngit clone https://huggingface.co/spaces/tonyjustdevs/pets_breed_predictor"
  },
  {
    "objectID": "posts/2024-01-25-98_huggingface/index.html#gather-your-files",
    "href": "posts/2024-01-25-98_huggingface/index.html#gather-your-files",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 3)",
    "section": "2. Gather your files",
    "text": "2. Gather your files\nRecall the various files we needed to run the app locally part 2.\nGather into the cloned huggingface folder:\n- Learner (.pkl)\n- Pet examples (pets.jpg)\n- Gradio app (app.py)\n\nA good way to check for me is seeing the pets_breed_predictor is the git folder and huggingface space."
  },
  {
    "objectID": "posts/2024-01-25-98_huggingface/index.html#push-to-huggingface-repo",
    "href": "posts/2024-01-25-98_huggingface/index.html#push-to-huggingface-repo",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 3)",
    "section": "3. Push to HuggingFace Repo",
    "text": "3. Push to HuggingFace Repo\nIf you‚Äôve pushed succesfully your app (could take several minutes), then your app is live! Congrats!\nIf you‚Äôre like me and forgot to include the requirements.txt then you‚Äôll be greeted with this error.\n\n\n3.1 Add the requirements.txt\nWe imported two libraries fastai and gradio so include them in the requirements.txt file. Commit and push."
  },
  {
    "objectID": "posts/2024-01-25-98_huggingface/index.html#web-app-complete-and-is-live",
    "href": "posts/2024-01-25-98_huggingface/index.html#web-app-complete-and-is-live",
    "title": "How to Build and Deploy a Deep-Learning Multi-Classifier Web App (Part 3)",
    "section": "4 Web App Complete and is Live",
    "text": "4 Web App Complete and is Live\nIf all goes well, the HuggingFace space is hosting the Gradio App!\nCheck out my web app here!"
  },
  {
    "objectID": "posts/2024-01-27-99_kaggle_api/index.html",
    "href": "posts/2024-01-27-99_kaggle_api/index.html",
    "title": "How To Setup a Kaggle API",
    "section": "",
    "text": "I‚Äôm planning to learn and test myself with competitions on Kaggle.\nKaggle is a place with real-world problems where Data Scientists and alike can go against each other to solve problems with Machine Learning.\nFrom my understanding:\n- There is a validation set where your model is tested against and a public leader board to see how you‚Äôre going.\n- At the end of the competition, there is an unseen test set where everyones models is tested against and where the final rankings are determined.\n- This is quite reflective of real world where preparing a representative validation set is vital, thus will perform well on the test set.\n- A common mistake for newbs is over-fitting to the validation set. I‚Äôm ready to make that mistake ü§£.\nAs for the Kaggle API, you can download the kernel which is the necessary datasets and source files to do the competitions.\nAlternatively, I can use the notebooks on their website. I plan to try doing competitions both ways.\nThis is how I set up my Kaggle API"
  },
  {
    "objectID": "posts/2024-01-27-99_kaggle_api/index.html#install-library",
    "href": "posts/2024-01-27-99_kaggle_api/index.html#install-library",
    "title": "How To Setup a Kaggle API",
    "section": "1. Install Library",
    "text": "1. Install Library\npip install python"
  },
  {
    "objectID": "posts/2024-01-27-99_kaggle_api/index.html#create-api-token-.json-file",
    "href": "posts/2024-01-27-99_kaggle_api/index.html#create-api-token-.json-file",
    "title": "How To Setup a Kaggle API",
    "section": "2. Create API token (.json file)",
    "text": "2. Create API token (.json file)\n\nGo to Kaggle\n\nGo to Settings\n\nCreate New Token"
  },
  {
    "objectID": "posts/2024-01-27-99_kaggle_api/index.html#save-to-your-local-.kaggle-folder-windows",
    "href": "posts/2024-01-27-99_kaggle_api/index.html#save-to-your-local-.kaggle-folder-windows",
    "title": "How To Setup a Kaggle API",
    "section": "3. Save to your local .kaggle folder (Windows)",
    "text": "3. Save to your local .kaggle folder (Windows)\nLocation: C:\\Users\\&lt;Windows-username&gt;\\.kaggle\\kaggle.json\nKaggle Github Reference\n\n3.1 Pasted into the wrong folder?\nIf you did something wrong then ran kaggle in the terminal, you‚Äôll an error (telling you where to put it):"
  },
  {
    "objectID": "posts/2024-01-27-99_kaggle_api/index.html#start-kaggling",
    "href": "posts/2024-01-27-99_kaggle_api/index.html#start-kaggling",
    "title": "How To Setup a Kaggle API",
    "section": "4 Start Kaggling",
    "text": "4 Start Kaggling\n\nimport kaggle\n??kaggle\n\nType:        module\nString form: &lt;module 'kaggle' from 'c:\\\\Users\\\\tonyp\\\\miniconda3\\\\envs\\\\fastai\\\\Lib\\\\site-packages\\\\kaggle\\\\__init__.py'&gt;\nFile:        c:\\users\\tonyp\\miniconda3\\envs\\fastai\\lib\\site-packages\\kaggle\\__init__.py\nSource:     \n#!/usr/bin/python\n#\n# Copyright 2024 Kaggle Inc\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# coding=utf-8\nfrom __future__ import absolute_import\nfrom kaggle.api.kaggle_api_extended import KaggleApi\nfrom kaggle.api_client import ApiClient\n\napi = KaggleApi(ApiClient())\napi.authenticate()"
  },
  {
    "objectID": "posts/2024-02-02-neural_network_basics/index.html",
    "href": "posts/2024-02-02-neural_network_basics/index.html",
    "title": "Neural Network Basics (Part 2)",
    "section": "",
    "text": "Automation of finding the best parameters (lowest loss) based on Mean Average Error (MAE) using Gradient Descent for our Quadratic Function"
  },
  {
    "objectID": "posts/2024-02-02-neural_network_basics/index.html#import-libraries",
    "href": "posts/2024-02-02-neural_network_basics/index.html#import-libraries",
    "title": "Neural Network Basics (Part 2)",
    "section": "1. Import Libraries",
    "text": "1. Import Libraries\n\nfrom ipywidgets import interact\nfrom fastai.basics import *\nimport pandas as pd\nfrom functools import partial"
  },
  {
    "objectID": "posts/2024-02-02-neural_network_basics/index.html#upload-data-and-convert-data-to-pytorch-tensors",
    "href": "posts/2024-02-02-neural_network_basics/index.html#upload-data-and-convert-data-to-pytorch-tensors",
    "title": "Neural Network Basics (Part 2)",
    "section": "2. Upload Data and Convert Data to Pytorch Tensors",
    "text": "2. Upload Data and Convert Data to Pytorch Tensors\n\ndf = pd.read_csv(\"upload_dataset.csv\")\nx_trch = torch.tensor(df.x) \ny_trch = torch.tensor(df.y)"
  },
  {
    "objectID": "posts/2024-02-02-neural_network_basics/index.html#create-customisable-quadratic-functions-and-interactively-plot-with-mae",
    "href": "posts/2024-02-02-neural_network_basics/index.html#create-customisable-quadratic-functions-and-interactively-plot-with-mae",
    "title": "Neural Network Basics (Part 2)",
    "section": "3. Create Customisable Quadratic functions and Interactively Plot with MAE",
    "text": "3. Create Customisable Quadratic functions and Interactively Plot with MAE\n\ndef gen_quad_fn(a,b,c,x): return a*x**2 + b*x + c\ndef custom_quad_fn(a,b,c): return partial(gen_quad_fn,a,b,c)\ndef torch_mae(prediction, actual): return (torch.abs(prediction-actual).mean())\ndef torch_mse(prediction, actual): return ((prediction-actual)**2).mean()\n\n\n# def mae(prediction, actual): return np.mean(abs(prediction-actual))\n# def torch_mae(prediction, actual): return np.mean(torch.abs(prediction-actual))\n# def mae(prediction, actual): return (torch.abs(prediction-actual).mean())\n# def mae2(prediction, actual): return abs(prediction-actual).mean()\n# def mae_jh(prediction, actual): return (abs(prediction-actual)).mean()\n# def mse_jh(prediction, actual): return ((prediction-actual)**2).mean()\n# def mae(preds, acts): return (torch.abs(preds-acts)).mean()\n\n\nplt.rc('figure', dpi=90)\n\n@interact(a=(0,2.1,0.1),b=(0,2.1,0.1),c=(0,2.1,0.1))\ndef interactive_plot(a,b,c):\n# 1.    plot scatter\n    plt.scatter(x_trch, y_trch)\n# 2     create custom_quad_interactive_fn\n# 2.1   create xs_interact    \n    xs_interact = x_trch\n# 3.    create ys_interact\n    plt.ylim(-1,15)\n    ys_interact = custom_quad_fn(a,b,c)(xs_interact)\n# 4.    calc mae\n    y_actual     = y_trch\n    y_predicted  = custom_quad_fn(a,b,c)(x_trch)\n    interact_mae = torch_mae(y_predicted,y_actual)\n# 5. plot   \n    plt.plot(xs_interact, ys_interact)\n    plt.title(f\"MAE: {interact_mae:.2f}\")"
  },
  {
    "objectID": "posts/2024-02-02-neural_network_basics/index.html#determining-the-effect-of-the-parameters-a-b-c-in-ax-bx2-c",
    "href": "posts/2024-02-02-neural_network_basics/index.html#determining-the-effect-of-the-parameters-a-b-c-in-ax-bx2-c",
    "title": "Neural Network Basics (Part 2)",
    "section": "4. Determining the effect of the parameters (\\(a\\), \\(b\\), \\(c\\)) in: \\(ax + bx^2 + c\\)",
    "text": "4. Determining the effect of the parameters (\\(a\\), \\(b\\), \\(c\\)) in: \\(ax + bx^2 + c\\)\nThe key thing to understand if whether the loss function gets better or worse when you increase the parameters a little.\nThere are two ways we can try: 1. Manually adjust the parameter: Move each parameter each way and observe the impact to MAE.\n2. Calculate the Derivative of the parameter: A Derivative iS a function that tells you if you increase the input the: - direction in which output changes (increases or decreases) and the;\n- magnitude of the change to the output\n\n4.1 Create Mean-Absolute-Error (mae) Quadratic Function\nThis function will take in the parameters or coefficients of a quadratic function and output the MSE. - Input: coeffiicents of quadratic - Output: MAE (between the prediction of the quadratic with the coffecients of the quadratic and the actual predictsions)\n\ndef mae_quad_fn(x_trch, y_trch, abc_params):\n    quad_fn = custom_quad_fn(*abc_params)\n    y_predicted_trch = quad_fn(x_trch)\n    y_actual_trch    = y_trch\n    # so quad_params(2,3,4) -&gt;  creates a custom quad fn -&gt; 2x^2 + 3x + 4\n    return torch_mae(y_predicted_trch,y_actual_trch)\n\ndef mse_quad_fn(x_trch, y_trch, abc_params):\n    quad_fn = custom_quad_fn(*abc_params)\n    y_predicted_trch = quad_fn(x_trch)\n    y_actual_trch    = y_trch\n    # so quad_params(2,3,4) -&gt;  creates a custom quad fn -&gt; 2x^2 + 3x + 4\n    return torch_mse(y_predicted_trch,y_actual_trch)\n\nThe chart shows MAE(2,2,2) = 1.4501 loss Our mae_function also calculates 1.491 loss.\n\nmae_quad_fn(x_trch=x_trch,y_trch=y_trch,abc_params=[1.0,1.0,1.0])\n\ntensor(2.6103, dtype=torch.float64)\n\n\nA tensor is a pytorch type that works with: - lists (1D tensors) - tables (2D tensors) - layers of tables of numbers (3D tensors) and etc\n\n\n4.2 Telling PyTorch to calculate gradients\nBy calling method .requires_grad_(), our abc_rg tensor is not will calculate gradients whenever we use the tensor.\n\n# rank 1 tensor\nabc_rg = torch.tensor([1.0,1.0,1.0])\nabc_rg.requires_grad_()\n\ntensor([1., 1., 1.], requires_grad=True)\n\n\n\nabc_rg\n\ntensor([1., 1., 1.], requires_grad=True)\n\n\n\n4.2.1 Method .requires_grad_()\ngrad_fn=&lt;MeanBackward0&gt; shows the gradients are calculated to for each parameter (our inputs)\n\nloss = mae_quad_fn(x_trch, y_trch, abc_rg)\nloss\n\ntensor(2.6103, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\n\n4.2.2 Method .backward()\nThis adds an attribute .grad to our abc_rg tensor.\n\nloss.backward()\n\n\n\n4.2.3 Attribute .grad\nThis attributes tells us if we increase the input slightly in the same position of this tensor, the loss will increase (if its positive) or decrease (if negative)\n\nabc_rg.grad\n\ntensor([-1.3529, -0.0316, -0.5000])\n\n\n\n\n4.2.4 Increase our abc parameters and recalculate loss\n\nwith torch.no_grad():\n    print(f\"loss before: {loss}\")\n    abc_rg -= abc_rg.grad * 0.01\n    loss = mae_quad_fn(x_trch, y_trch, abc_rg)\n    print(f\"loss after: {loss}\")\n\nloss before: 2.61030324932801\nloss after: 2.5894896953092177\n\n\n\n\n4.2.5 Automate it\nCreate a loop that decreases the loss by iteratively increasing the parameters (since the gradients are negative, or vice versa)\n\nfor i in range(10):\n    loss = mae_quad_fn(x_trch, y_trch, abc_rg)\n    loss.backward()\n    with torch.no_grad(): abc_rg -= abc_rg.grad * 0.01\n    print(f\"step {i}: {loss} - {abc_rg.grad}\") \n\nstep 0: 2.5894896953092177 - tensor([-2.7058, -0.0632, -1.0000])\nstep 1: 2.547862587271633 - tensor([-4.0587, -0.0947, -1.5000])\nstep 2: 2.4854217639359875 - tensor([-5.4116, -0.1263, -2.0000])\nstep 3: 2.4021673865815485 - tensor([-6.7645, -0.1579, -2.5000])\nstep 4: 2.2980994552083187 - tensor([-8.1175, -0.1895, -3.0000])\nstep 5: 2.173217969816296 - tensor([-9.4704, -0.2211, -3.5000])\nstep 6: 2.0300959430578267 - tensor([-10.6892,  -0.3684,  -3.9000])\nstep 7: 1.883669135864714 - tensor([-11.9080,  -0.5158,  -4.3000])\nstep 8: 1.740979068220988 - tensor([-12.9396,  -0.8000,  -4.6000])\nstep 9: 1.5914231086209807 - tensor([-13.9712,  -1.0842,  -4.9000])\n\n\n\n\n\n5 Parameters are getting closer\nThe parameters started as 1,1,1 and now are 1.9, 1.0, 1.3, the underlying function was modelled with 3, 2, 1 so its getting there!\n[Future Iteration] How to just fix a parameter and just move the others?\n\nabc_rg\n\ntensor([1.8739, 1.0365, 1.3170], requires_grad=True)"
  },
  {
    "objectID": "posts/2024-02-02-neural_network_basics/index.html#to-be-continued",
    "href": "posts/2024-02-02-neural_network_basics/index.html#to-be-continued",
    "title": "Neural Network Basics (Part 2)",
    "section": "To be Continued‚Ä¶",
    "text": "To be Continued‚Ä¶\nNext A universal function called the ReLU Function (rather than a quadratric function) is used for our modelling.\nNeural Network Basics: Part 1\nNeural Network Basics: Part 2\nNeural Network Basics: Part 3"
  }
]